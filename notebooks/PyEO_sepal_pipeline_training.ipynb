{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c075ae52-fa2e-4490-acd5-99baaa8fa569",
   "metadata": {},
   "source": [
    "# PyEO Processing: How to build a change detection pipeline using your classification model.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8acb1d3-96a5-452a-8914-b176287f3ae7",
   "metadata": {
    "tags": []
   },
   "source": [
    "- Here we will cover how to query and download images in order to build a cloud-free composite baseline\n",
    "- We can then use this a reference against which to compare later 'change' images and so be able to identify forest changes\n",
    "- Identified forest changes can then be processed and filtered based on the pattern over time\n",
    "- Manual assessment can then be judge which alert ares to flag for further investigation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5bd835",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Composite Baseline Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af63284",
   "metadata": {},
   "source": [
    "- This section will take us stepwise through the imagery query, download and composite creation aspects of the `run_acd_national.py` script, which runs the full PyEO pipeline from the command line in a terminal.  \n",
    "- Jupyter notebooks provide a useful and engaging interface to understand the components of this script, so we will follow an extracted version throughout this notebook.\n",
    "\n",
    "This section comprises several stages:   \n",
    "1. Directory and V=variable setup.\n",
    "1. Querying for Sentinel-2 imagery that meets our search criteria.\n",
    "1. Downloading the Sentinel-2 imagery identified from the Query.\n",
    "1. If necessary, preprocess any L1C to L2A by applying atmospheric corrections. \n",
    "1. Cloud-masking the L2A imagery.\n",
    "1. Creation of a composite baseline reference from the time series that has been downloaded and processed. \n",
    "1. Query and Download of a set of Change Detection Images\n",
    "1. Classification of Baseline and Change Detection Images\n",
    "1. Creation of Forest Alerts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea452e7-c5a0-4d5f-9cda-82333f2b1545",
   "metadata": {},
   "source": [
    "# Setup: Requirements to use this Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a22d9d0-a503-479a-96bc-02936a3ffd7b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Select the virtual environment\n",
    "- Use the drop-down list at the top right of the Jupyter notebook window\n",
    "- Select (venv) Python for Earth Observation (PyEO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f568cf49-cf14-4570-aba1-0183d91c51fa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Check the working directory is set to `pyeo` within your PyEO installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ae2c979-6be0-4acc-a539-e31325b49399",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/sepal-user/20230626_pyeo_installation/pyeo/notebooks'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8790ae9-e63c-46cd-b520-0819bd97a5b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sepal-user/20230626_pyeo_installation/pyeo\n"
     ]
    }
   ],
   "source": [
    "cd /home/sepal-user/20230626_pyeo_installation/pyeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d1f18a3-e8b9-4263-af67-71013835a896",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/sepal-user/20230626_pyeo_installation/pyeo'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4498fd70",
   "metadata": {},
   "source": [
    "## <a id='toc3_1_'></a>[Directory and Variable Setup](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a867b3a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <a id='toc3_1_1_'></a>[Import Libraries](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "423f8dec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries successfully imported\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import sys\n",
    "\n",
    "from pyeo import (classification, filesystem_utilities,\n",
    "                    queries_and_downloads, raster_manipulation)\n",
    "\n",
    "from pyeo.acd_national import (acd_initialisation,\n",
    "                                 acd_config_to_log,\n",
    "                                 acd_roi_tile_intersection)\n",
    "import configparser\n",
    "import argparse\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "from osgeo import gdal\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import zipfile\n",
    "\n",
    "gdal.UseExceptions()\n",
    "\n",
    "print(\"Libraries successfully imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91268ca5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <a id='toc3_1_3_'></a>[Declare Processing Parameters with In-Notebook Variables](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b45b00b-0602-45ce-b783-2644c377cc20",
   "metadata": {},
   "source": [
    "- When running the `pyeo` pipeline we use an initialisation file (.ini) to provide the required parameters.  \n",
    "- For SEPAL, we will use `pyeo_sepal.ini`  \n",
    "- Below the parameters are explained, but we will read these in via the config parser."
   ]
  },
  {
   "cell_type": "raw",
   "id": "6c9a27b5-8957-442b-aba9-c357068d5526",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Acquisition dates for Images in the Change Period, in the form yyyymmdd\n",
    "start_date=20230101  \n",
    "end_date=20230611\n",
    "\n",
    "# Acquisition dates for Images for the Baseline Composite, in the form yyyymmdd\n",
    "composite_start=20220101\n",
    "composite_end=20221231\n",
    "\n",
    "# EPSG code for Kenya - north of equator and east of 36Â°E is EPSG:21097\n",
    "# See https://epsg.io/21097 and https://spatialreference.org/ref/epsg/21097/\n",
    "epsg=21097\n",
    "\n",
    "# Cloud cover threshold for imagery to download\n",
    "cloud_cover=25\n",
    "\n",
    "# Certainty value above which a pixel is considered a cloud from sen2cor\n",
    "cloud_certainty_threshold=0\n",
    "\n",
    "# path to the trained machine learning model for land cover in Kenya\n",
    "model= \"./models/model_36MYE_Unoptimised_20230505_no_haze.pkl\"\n",
    "\n",
    "# pyeo_dir needs to be an absolute path\n",
    "pyeo_dir = \"/home/sepal-user/module-venv/pyeo\"\n",
    "# tile_dir needs to be an absolute path\n",
    "tile_dir = \"/home/sepal-user/pyeo_downloads\"\n",
    "\n",
    "# Relative paths are relative to pyeo_dir\\\n",
    "integrated_dir = \"./integrated\"\n",
    "roi_dir = \"./roi\"\n",
    "roi_filename = \"kfs_roi_subset_c.shp\"\n",
    "geometry_dir = \"./geometry\"\n",
    "s2_tiles_filename = \"kenya_s2_tiles.shp\"\n",
    "log_dir = \"./log\"\n",
    "log_filename = \"test_sepal_venv_20230614.log\"\n",
    "credentials_path = \"/home/sepal-user/module-venv/credentials/credentials.ini\"\n",
    "\n",
    "environment_manager = \"venv\"\n",
    "\n",
    "# Path to the sen2cor preprocessor script, L2A_Process. Usually in the bin/ folder of your sen2cor installation.\n",
    "sen2cor_path = \"/bin/L2A_Process\"\n",
    "\n",
    "# ***** STEP 1 ROI-TILE INTERSECTION  *****\n",
    "do_tile_intersection = True\n",
    "# **************************************************************************************************************************\n",
    "\n",
    "# ***** STEP 2 SETUP GENERAL RASTER PROCESSING OPTIONS ***** \n",
    "do_raster = True\n",
    "chunks = 10\n",
    "do_skip_existing = True\n",
    "do_quicklooks = False\n",
    "# **************************************************************************************************************************\n",
    "\n",
    "# ***** STEP 3 SETUP GENERAL RASTER PROCESSING PARAMETERS ***** \n",
    "download_source = \"scihub\"\n",
    "# granules below this size in MB will not be downloaded\n",
    "faulty_granule_threshold = 350\n",
    "# list of strings with the band name elements of the image file names in \"\" string notation\n",
    "# the wavebands specified here must match those used to build the random forest model specified in the Classify section below\n",
    "band_names = [\"B02\", \"B03\", \"B04\", \"B08\"]\n",
    "# file name pattern to search for when identifying band file locations in \"\" string notation\n",
    "resolution_string = \"10m\"\n",
    "# spatial resolution of the output raster files in metres. Can be any resolution, not just 10, 20 or 60 as in the default band resolutions of Sentinel-2\n",
    "output_resolution = 10\n",
    "# **************************************************************************************************************************\n",
    "\n",
    "\n",
    "# ***** STEP 4 DOWNLOAD REFERENCE IMAGES AND BUILD A MEDIAN COMPOSITE ***** \n",
    "do_build_composite = True\n",
    "# set buffer in number of pixels for dilating the SCL cloud mask (recommend 10 pixels of 10 m) for the composite building\n",
    "buffer_size_cloud_masking_composite = 10\n",
    "# maximum number of images to be downloaded for compositing, in order of least cloud cover\n",
    "download_limit = 5\n",
    "# **************************************************************************************************************************\n",
    "\n",
    "\n",
    "# ***** STEP 5: DOWNLOAD CHANGE DETECTION IMAGES FOR THE REQUIRED DATE RANGE ***** \n",
    "do_download = True\n",
    "# set buffer in number of pixels for dilating the SCL cloud mask (recommend 30 pixels of 10 m) for the change detection\n",
    "buffer_size_cloud_masking = 20\n",
    "# **************************************************************************************************************************\n",
    "\n",
    "\n",
    "# ***** STEP 6: CLASSIFIY THE COMPOSITE AND CHANGE DETECTION IMAGES ***** \n",
    "# ***** NOTE: MOVE MODEL SPECIFICATION TO THIS SECTION***** \n",
    "do_build_prob_image = False \n",
    "# do_build_prob_image, consider removing\n",
    "do_classify = True\n",
    "# list of strings with class labels starting from class 1. Must match the trained model that was used.\n",
    "class_labels = [\"primary forest\", \"plantation forest\", \"bare soil\", \"crops\", \"grassland\", \"open water\", \"burn scar\", \"cloud\", \"cloud shadow\", \"haze\", \"sparse woodland\", \"dense woodland\", \"artificial\"]\n",
    "# if sieve is 0, no sieve is applied. If >0, the classification images will be sieved using gdal and all contiguous groups of pixels smaller than this number will be eliminated\n",
    "sieve = 0\n",
    "# **************************************************************************************************************************\n",
    "\n",
    "\n",
    "# ***** STEP 7: DETECT CHANGES AND BUILD RASTER REPORTS ***** \n",
    "# ***** NOTE: ADD REPORT BINARISATION PARAMETERS TO THIS SECTION MIN_DETECTION; CONSISTENCY ETC.***** \n",
    "do_change = True\n",
    "# find subsequent changes from any of these classes. Must match the trained model that was used.\n",
    "change_from_classes = [1, 2]\n",
    "# to any of these classes. Must match the trained model that was used.\n",
    "change_to_classes = [3]\n",
    "# **************************************************************************************************************************\n",
    "\n",
    "\n",
    "# ***** STEP 8: VECTOR ANALYSIS OF TILE RASTER REPORTS ***** \n",
    "[vector_processing_parameters]\n",
    "level_1_filename = \"gadm41_KEN_1.json\"\n",
    "# vectorisation currently hardcoded to use level_1_filename\n",
    "level_2_filename = \"gadm41_KEN_2.json\"\n",
    "level_3_filename = \"gadm41_KEN_3.json\"\n",
    "\n",
    "do_delete_existing_vector = True\n",
    "do_vectorise = True\n",
    "\n",
    "\n",
    "# ***** STEP 9: INTEGRATE VECTOR ANALYSES TO NATIONAL SCOPE ***** \n",
    "do_integrate = True\n",
    "# **************************************************************************************************************************\n",
    "\n",
    "# ***** STEP 10: FILTER NATIONAL SCOPE VECTORISED FOREST ALERTS ***** \n",
    "do_filter = False\n",
    "# If there are any strings within counties_of_interest list, filtering by county will be attempted\n",
    "counties_of_interest = []\n",
    "# Counties_of_interest = [\"Kwale\", \"TransNzoia\"]\n",
    "minimum_area_to_report_m2 = 120"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2633ca",
   "metadata": {},
   "source": [
    "- Now, let's read in the `.ini` file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ed25ff-9f70-4b31-853c-395c0c3b73a3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Declare the path to the initialisation file\n",
    "- You can leave this path alone if you are using the `pyeo_sepal.ini` file that comes packaged with PyEO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ee703a3-ca73-47ba-b7df-34ce92ed0fe9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/sepal-user/20230626_pyeo_installation/pyeo'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d943d13-0f65-4697-b9f9-0de863748885",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config_path = \"/home/sepal-user/20230626_pyeo_installation/pyeo/pyeo_sepal.ini\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef0e050-60ea-44aa-adbb-984ed605e4a4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Edit the `pyeo_sepal.ini` file\n",
    "You can either:  \n",
    "- Check that `pyeo_dir` and `tile_dir` in `pyeo_sepal.ini` match those below. These paths should be the same as below if you have followed the instructions in `pyeo_sepal_orientation` notebook\"\n",
    "    - ```pyeo_dir = /home/sepal-user/20230626_pyeo_installation/pyeo```\n",
    "    - ```tile_dir = /home/sepal-user/20230626_pyeo_installation```  \n",
    "    \n",
    "    <br>\n",
    "- Or, amend the `pyeo_sepal.ini` file to match your file paths if you cloned `pyeo` into a different directory:\n",
    "    - Right-Click and 'Open' pyeo_sepal.ini in the file browser on the left to be able to edit it\n",
    "    - Change pyeo_dir to point to the pyeo code in your installation directory\n",
    "    ```pyeo_dir = /home/sepal-user/20230626_pyeo_installation/pyeo```\n",
    "    - Change tile_dir to point to your installation directory, this is where data files will be stored\n",
    "    ```tile_dir = /home/sepal-user/20230626_pyeo_installation```\n",
    "    - Save the edited initialisation file - by pressing ```Ctrl+S```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c21844-9c03-4e28-8316-4f57fca827e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Edit the `credentials_dummy.ini` file:\n",
    "- Ensure the credentials path in the `pyeo_sepal.ini` corresponds to your credentials file.\n",
    "- The default path is to `credentials/credentials.ini`\n",
    "- To use this default option open the file `credentials_dummy.ini` in the editor (Right-Click then 'Open')\n",
    "- Edit the file to add your personal credentials for the dataspace API - following the convention of this file.\n",
    "- Save the file as `credentials.ini` into the credentials folder (using File -> Save File As)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8c80083a-43fa-4632-920c-2bfdfd11b600",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-29 08:21:19,959: INFO: ---------------------------------------------------------------\n",
      "2023-06-29 08:21:19,960: INFO:                     ****PROCESSING START****\n",
      "2023-06-29 08:21:19,961: INFO: ---------------------------------------------------------------\n",
      "2023-06-29 08:21:19,962: INFO: ---------------------------------------------------------------\n",
      "2023-06-29 08:21:19,963: INFO: ---                  INTEGRATED PROCESSING START            ---\n",
      "2023-06-29 08:21:19,964: INFO: ---------------------------------------------------------------\n",
      "2023-06-29 08:21:19,964: INFO: Reading in parameters defined in: /home/sepal-user/20230626_pyeo_installation/pyeo/pyeo_sepal.ini\n",
      "2023-06-29 08:21:19,965: INFO: ---------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "config_dict, acd_log = acd_initialisation(config_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ed6332-bbf5-42fb-b1bb-870a05275558",
   "metadata": {},
   "source": [
    "### Read the `.ini` file into Python\n",
    "- Here we create a variable called `config_dict`, which is a Python dictionary containing the configuration parameters we explored in one of the cells above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9387bb44-7a6f-4bbf-9bfa-099c80f50d5b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/sepal-user/20230626_pyeo_installation/pyeo'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7312a4a-5c13-4210-8493-bbfa38ea17ae",
   "metadata": {},
   "source": [
    "### Print the configuration parameters\n",
    "- We print the configuration parameter to create a record of what pyeo has been configured to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1cd5ccfe-a8d9-4097-9218-336c298ed1a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-29 08:21:20,093: INFO: Options:\n",
      "2023-06-29 08:21:20,093: INFO: The Environment Manager configured to use is : venv\n",
      "2023-06-29 08:21:20,094: INFO:   --dev Running in development mode, choosing development versions of functions where available\n",
      "2023-06-29 08:21:20,095: INFO:   -do_tile_intersection\n",
      "2023-06-29 08:21:20,095: INFO:       Sentinel-2 tile intersection with ROI enabled\n",
      "2023-06-29 08:21:20,096: INFO:   --do_raster\n",
      "2023-06-29 08:21:20,098: INFO:       raster pipeline enabled\n",
      "2023-06-29 08:21:20,099: INFO:   --quicklooks to create image quicklooks\n",
      "2023-06-29 08:21:20,099: INFO:   --do_delete_existing_vector , when vectorising the change report rasters, \n",
      "2023-06-29 08:21:20,100: INFO:             existing vectors files will be deleted and new vector files created.\n",
      "2023-06-29 08:21:20,100: INFO:   --do_vectorise\n",
      "2023-06-29 08:21:20,101: INFO:       raster change reports will be vectorised\n",
      "2023-06-29 08:21:20,102: INFO: EPSG used is: 21097\n",
      "2023-06-29 08:21:20,102: INFO: List of image bands: ['B02', 'B03', 'B04', 'B08']\n",
      "2023-06-29 08:21:20,103: INFO: Model used: /home/sepal-user/mato_rf_toby_1_ir_pkl_converted.pkl\n",
      "2023-06-29 08:21:20,105: INFO: \n",
      "2023-06-29 08:21:20,106: INFO: List of class labels:\n",
      "2023-06-29 08:21:20,107: INFO:   1 : primary forest\n",
      "2023-06-29 08:21:20,107: INFO:   2 : plantation forest\n",
      "2023-06-29 08:21:20,108: INFO:   3 : bare soil\n",
      "2023-06-29 08:21:20,110: INFO:   4 : crops\n",
      "2023-06-29 08:21:20,110: INFO:   5 : grassland\n",
      "2023-06-29 08:21:20,111: INFO:   6 : open water\n",
      "2023-06-29 08:21:20,112: INFO:   7 : burn scar\n",
      "2023-06-29 08:21:20,112: INFO:   8 : cloud\n",
      "2023-06-29 08:21:20,113: INFO:   9 : cloud shadow\n",
      "2023-06-29 08:21:20,114: INFO:   10 : haze\n",
      "2023-06-29 08:21:20,114: INFO:   11 : sparse woodland\n",
      "2023-06-29 08:21:20,115: INFO:   12 : dense woodland\n",
      "2023-06-29 08:21:20,116: INFO:   13 : artificial\n",
      "2023-06-29 08:21:20,116: INFO: Detecting changes from any of the classes: [1, 2]\n",
      "2023-06-29 08:21:20,117: INFO:                     to any of the classes: [3, 4, 5, 7, 11, 12, 13]\n",
      "2023-06-29 08:21:20,118: INFO: ------------------------------------------------------------\n",
      "2023-06-29 08:21:20,118: INFO: Reporting Directories and Filepaths\n",
      "2023-06-29 08:21:20,119: INFO: Tile Directory is   : /home/sepal-user/20230626_pyeo_installation\n",
      "2023-06-29 08:21:20,120: INFO: Working Directory is   : /home/sepal-user/20230626_pyeo_installation/pyeo\n",
      "2023-06-29 08:21:20,120: INFO: The following directories are relative to the working directory, i.e. stored underneath:\n",
      "2023-06-29 08:21:20,121: INFO: Integrated Directory is   : ./integrated\n",
      "2023-06-29 08:21:20,122: INFO: ROI Directory is   : ./roi\n",
      "2023-06-29 08:21:20,122: INFO: Geometry Directory is   : ./geometry\n",
      "2023-06-29 08:21:20,123: INFO: Path to the Administrative Boundaries used in the Change Report Vectorisation   : ./geometry/gadm41_KEN_1.json\n",
      "2023-06-29 08:21:20,124: INFO: Path to Sen2Cor is   : /bin/L2A_Process\n",
      "2023-06-29 08:21:20,124: INFO: -------------------------------------------\n",
      "2023-06-29 08:21:20,125: INFO: -------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "acd_config_to_log(config_dict, acd_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8668a8-6e76-47c3-b5d4-f58d69480016",
   "metadata": {},
   "source": [
    "## Identify the required Sentinel-2 tiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a800e86c-c409-48e3-a401-73bc7f75ca49",
   "metadata": {},
   "source": [
    "- PyEO operates by looking at a shapefile to determine the Region of Interest (ROI)\n",
    "- This directory path and filename of this shapefile needs to be specified in these two lines in `pyeo_sepal.ini`:\n",
    "    - `roi_dir = ./roi`\n",
    "    - `roi_filename = kfs_roi_subset_c.shp`\n",
    "- Then in the cell below, PyEO identifies what Sentinel-2 tiles intersect with the Region Of Interest (ROI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "399b4729-a380-4293-b9d2-1196f0ffe93a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-29 08:21:21,147: INFO: The provided ROI intersects with 2 Sentinel-2 tiles\n",
      "2023-06-29 08:21:21,148: INFO: These tiles are  :\n",
      "2023-06-29 08:21:21,149: INFO:   1 : 36NXG\n",
      "2023-06-29 08:21:21,150: INFO:   2 : 36NYG\n",
      "2023-06-29 08:21:21,150: INFO: Writing Sentinel-2 tiles that intersect with the provided ROI to  : ./roi/tilelist.csv\n",
      "2023-06-29 08:21:21,169: INFO: Finished ROI tile intersection\n"
     ]
    }
   ],
   "source": [
    "os.chdir(config_dict[\"pyeo_dir\"]) # ensures pyeo is looking in the correct directory\n",
    "tilelist_filepath = acd_roi_tile_intersection(config_dict, acd_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f83faf41-ed28-4f86-b15a-510df4da74e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./roi/tilelist.csv\n"
     ]
    }
   ],
   "source": [
    "print(tilelist_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec40461f-d885-4c7f-8dbf-e16711ca6902",
   "metadata": {},
   "source": [
    "- **Right-Click on tile_list.csv in the JupyterLab explorer to the left and select 'open' to view it in a tab within JupyterLab.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213f2d20-0ca7-43ae-b6ab-1b0fa8f3c75e",
   "metadata": {},
   "source": [
    "##Â Running PyEO Per-Tile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4161a3fd-652d-46cb-ba75-12df4a5df04d",
   "metadata": {},
   "source": [
    "- PyEO is designed to run per-tile.\n",
    "- It takes `tilelist.csv` created in the above cell and runs the pipeline for each tile in this `.csv` file.\n",
    "- This tutorial will run through the pipeline for the first tile in `tilelist.csv` : `36NXG`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb06594",
   "metadata": {},
   "source": [
    "## <a id='toc3_1_4_'></a>[Create the Folder Structure PyEO Expects](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "245cb708",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder structure build successfully finished\n"
     ]
    }
   ],
   "source": [
    "os.chdir(config_dict[\"pyeo_dir\"]) # ensures pyeo is looking in the correct directory\n",
    "\n",
    "tile_to_process = pd.read_csv(tilelist_filepath)[\"tile\"][0]\n",
    "individual_tile_directory_path = os.path.join(config_dict[\"tile_dir\"], tile_to_process)\n",
    "filesystem_utilities.create_folder_structure_for_tiles(individual_tile_directory_path)\n",
    "print(\"Folder structure build successfully finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261c5ebc-dec2-436a-b7ff-4b9c10ee7973",
   "metadata": {},
   "source": [
    "- You can now use the JupyterLab file explorer to view the new folder structure which should be in your installation directory and called `36NXG`\n",
    "- These folders provide the skeleton for the pipeline to store and process the tile's imagery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "236f4703-f869-4a5e-812d-27ac679a15d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/sepal-user/20230626_pyeo_installation/36NXG'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "individual_tile_directory_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776dc55b",
   "metadata": {},
   "source": [
    "## <a id='toc3_1_5_'></a>[Create the Log File](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c7c121",
   "metadata": {},
   "source": [
    "- `PyEO` uses a Log file as a convenient location to monitor pipeline progress\n",
    "- Additionally, the log file acts as a record of which parameters were used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "616d0ab2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-29 08:21:21,424: INFO: ---------------------------------------------------------------\n",
      "2023-06-29 08:21:21,425: INFO:                     ****PROCESSING START****\n",
      "2023-06-29 08:21:21,426: INFO: ---------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "tile_log = filesystem_utilities.init_log_acd(\n",
    "    log_path=os.path.join(individual_tile_directory_path, \"log\", tile_to_process + \".log\"),\n",
    "    logger_name=f\"pyeo_{tile_to_process}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684df524-0e73-4148-9339-7898adb6a690",
   "metadata": {},
   "source": [
    "- You can now use the JupyterLab file explorer to find the log file which will be in a log folder beneath the main tile directory\n",
    "    - For example, the path to the Log file for `36NXG`, is : `20230626_pyeo_installation/36NXG/log/36NXG.log`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e48b98f-e301-4064-9fd2-6e6e2ba8bfe9",
   "metadata": {},
   "source": [
    "## Create the Required Directory Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c486eb",
   "metadata": {},
   "source": [
    "- This cell ensures the paths for the directory strucutre are available to pipeline commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "37d34faa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-29 08:21:21,493: INFO: Creating the directory paths\n",
      "2023-06-29 08:21:21,495: INFO: Successfully created the directory paths\n"
     ]
    }
   ],
   "source": [
    "tile_log.info(\"Creating the directory paths\")\n",
    "\n",
    "change_image_dir = os.path.join(individual_tile_directory_path, r\"images\")\n",
    "l1_image_dir = os.path.join(individual_tile_directory_path, r\"images/L1C\")\n",
    "l2_image_dir = os.path.join(individual_tile_directory_path, r\"images/L2A\")\n",
    "l2_masked_image_dir = os.path.join(individual_tile_directory_path, r\"images/cloud_masked\")\n",
    "categorised_image_dir = os.path.join(individual_tile_directory_path, r\"output/classified\")\n",
    "probability_image_dir = os.path.join(individual_tile_directory_path, r\"output/probabilities\")\n",
    "sieved_image_dir = os.path.join(individual_tile_directory_path, r\"output/sieved\")\n",
    "composite_dir = os.path.join(individual_tile_directory_path, r\"composite\")\n",
    "composite_l1_image_dir = os.path.join(individual_tile_directory_path, r\"composite/L1C\")\n",
    "composite_l2_image_dir = os.path.join(individual_tile_directory_path, r\"composite/L2A\")\n",
    "composite_l2_masked_image_dir = os.path.join(individual_tile_directory_path, r\"composite/cloud_masked\")\n",
    "quicklook_dir = os.path.join(individual_tile_directory_path, r\"output/quicklooks\")\n",
    "\n",
    "tile_log.info(\"Successfully created the directory paths\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c60c18d-8faf-4d5e-8f6e-a340a2072dc6",
   "metadata": {},
   "source": [
    "- So now, we can type any of the above variables into a Python cell and when we execute it - we will see the file path for that directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6a64198-0899-4049-a69f-9bf280b7e595",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/sepal-user/20230626_pyeo_installation/36NXG/images/L1C'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1_image_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00117de-7c4e-4f34-9086-b713215ca01c",
   "metadata": {},
   "source": [
    "## Create the Processing Argument Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce94b0d6-4948-4c06-abd3-e017cb93f0f0",
   "metadata": {},
   "source": [
    "- `PyEO` uses these parameters to make decisions throughout the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8258ae90-a781-4338-aeb4-554c46ba6c54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir(config_dict[\"pyeo_dir\"]) # ensures pyeo is looking in the correct directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6b9a4ca4-7858-4e8f-bfa3-41c04c4be795",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-29 08:21:21,751: INFO: dataspace API is the download source\n",
      "2023-06-29 08:21:21,752: INFO: Faulty Granule Threshold is set to   : 350\n",
      "2023-06-29 08:21:21,753: INFO:     Files below this threshold will not be downloaded\n",
      "2023-06-29 08:21:21,762: INFO: Successfully read the processing arguments and credentials\n"
     ]
    }
   ],
   "source": [
    "start_date = config_dict[\"start_date\"]\n",
    "end_date = config_dict[\"end_date\"]\n",
    "composite_start_date = config_dict[\"composite_start\"]\n",
    "composite_end_date = config_dict[\"composite_end\"]\n",
    "cloud_cover = config_dict[\"cloud_cover\"]\n",
    "cloud_certainty_threshold = config_dict[\"cloud_certainty_threshold\"]\n",
    "model_path = config_dict[\"model_path\"]\n",
    "sen2cor_path = config_dict[\"sen2cor_path\"]\n",
    "epsg = config_dict[\"epsg\"]\n",
    "bands = config_dict[\"bands\"]\n",
    "resolution = config_dict[\"resolution_string\"]\n",
    "out_resolution = config_dict[\"output_resolution\"]\n",
    "buffer_size = config_dict[\"buffer_size_cloud_masking\"]\n",
    "buffer_size_composite = config_dict[\"buffer_size_cloud_masking_composite\"]\n",
    "max_image_number = config_dict[\"download_limit\"]\n",
    "faulty_granule_threshold = config_dict[\"faulty_granule_threshold\"]\n",
    "download_limit = config_dict[\"download_limit\"]\n",
    "\n",
    "skip_existing = config_dict[\"do_skip_existing\"]\n",
    "sieve = config_dict[\"sieve\"]\n",
    "from_classes = config_dict[\"from_classes\"]\n",
    "to_classes = config_dict[\"to_classes\"]\n",
    "\n",
    "download_source = config_dict[\"download_source\"]\n",
    "if download_source == \"scihub\":\n",
    "    tile_log.info(\"scihub API is the download source\")\n",
    "if download_source == \"dataspace\":\n",
    "    tile_log.info(\"dataspace API is the download source\")\n",
    "\n",
    "tile_log.info(f\"Faulty Granule Threshold is set to   : {config_dict['faulty_granule_threshold']}\")\n",
    "tile_log.info(\"    Files below this threshold will not be downloaded\")\n",
    "\n",
    "credentials_path = config_dict[\"credentials_path\"]\n",
    "if not os.path.exists(credentials_path):\n",
    "    tile_log.error(f\"The credentials path does not exist  :{credentials_path}\")\n",
    "    tile_log.error(f\"Current working directory :{os.getcwd()}\")\n",
    "    tile_log.error(\"Exiting raster pipeline\")\n",
    "    sys.exit(1)\n",
    "\n",
    "conf = configparser.ConfigParser(allow_no_value=True, interpolation=None)\n",
    "conf.read(credentials_path)\n",
    "credentials_dict = {}\n",
    "\n",
    "tile_log.info(\"Successfully read the processing arguments and credentials\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b9894b-c798-4f53-a9ea-21511b88c57f",
   "metadata": {},
   "source": [
    "## Read the Specified Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "58aa1cde-169e-4824-b37a-45369e196a49",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-29 08:21:21,830: INFO: Running download handler for dataspace\n",
      "2023-06-29 08:21:21,831: INFO: Successfully configured the credentials for dataspace\n"
     ]
    }
   ],
   "source": [
    "if download_source == \"dataspace\":\n",
    "\n",
    "    tile_log.info(f'Running download handler for {download_source}')\n",
    "\n",
    "    credentials_dict[\"sent_2\"] = {}\n",
    "    credentials_dict[\"sent_2\"][\"user\"] = conf[\"dataspace\"][\"user\"]\n",
    "    credentials_dict[\"sent_2\"][\"pass\"] = conf[\"dataspace\"][\"pass\"]\n",
    "    sen_user = credentials_dict[\"sent_2\"][\"user\"]\n",
    "    sen_pass = credentials_dict[\"sent_2\"][\"pass\"]\n",
    "\n",
    "if download_source == \"scihub\":\n",
    "\n",
    "    tile_log.info(f'Running download handler for {download_source}')\n",
    "\n",
    "    credentials_dict[\"sent_2\"] = {}\n",
    "    credentials_dict[\"sent_2\"][\"user\"] = conf[\"sent_2\"][\"user\"]\n",
    "    credentials_dict[\"sent_2\"][\"pass\"] = conf[\"sent_2\"][\"pass\"]\n",
    "    sen_user = credentials_dict[\"sent_2\"][\"user\"]\n",
    "    sen_pass = credentials_dict[\"sent_2\"][\"pass\"]\n",
    "    \n",
    "tile_log.info(f\"Successfully configured the credentials for {download_source}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15ad6c3",
   "metadata": {},
   "source": [
    "## <a id='toc3_2_'></a>[Query Sentinel-2 Composite Imagery](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d0c510",
   "metadata": {},
   "source": [
    "First, a brief primer on the two Sentinel-2 data products we are concerned with:\n",
    "- L1C\n",
    "- L2A\n",
    "\n",
    "**L1C** corresponds to the 1st processing level for the imagery. <br>\n",
    "\n",
    "**L2A** corresponds to the 2nd processing level and this is the imagery we want to work with as these have been **atmospherically corrected**.\n",
    "\n",
    "-------------------------------\n",
    "\n",
    "Now that we have the query, file handling and log parameters set up, we can start querying the Copernicus Hub for the Sentinel-2 imagery that we want.  \n",
    "\n",
    "The cell below starts the `build_composite` process. First, we query for the `L1C` products that match our criteria (date range, tile of interest, cloud cover).\n",
    "\n",
    "Since we have declared a download limit of 12 images, the software caps the number of images in our query. This is a useful tool if we have limited disk space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02a8c720",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if config_dict[\"build_composite\"] or config_dict[\"do_all\"]:\n",
    "    tile_log.info(\"---------------------------------------------------------------\")\n",
    "    tile_log.info(\n",
    "        \"Creating an initial cloud-free median composite from Sentinel-2 as a baseline map\"\n",
    "    )\n",
    "    tile_log.info(\"---------------------------------------------------------------\")\n",
    "    tile_log.info(\"Searching for images for initial composite.\")\n",
    "\n",
    "    if download_source == \"dataspace\":\n",
    "\n",
    "        try:\n",
    "            tiles_geom_path = os.path.join(config_dict[\"pyeo_dir\"], os.path.join(config_dict[\"geometry_dir\"], config_dict[\"s2_tiles_filename\"]))\n",
    "            tile_log.info(f\"Path to the S2 tile geometry information absolute path: {os.path.abspath(tiles_geom_path)}\")\n",
    "            tiles_geom = gpd.read_file(os.path.abspath(tiles_geom_path))\n",
    "        except FileNotFoundError:\n",
    "            tile_log.error(f\"Path to the S2 tile geometry does not exist, absolute path given: {os.path.abspath(tiles_geom_path)}\")\n",
    "\n",
    "        tile_geom = tiles_geom[tiles_geom[\"Name\"] == tile_to_process]\n",
    "        tile_geom = tile_geom.to_crs(epsg=4326)\n",
    "        geometry = tile_geom[\"geometry\"].iloc[0]\n",
    "        geometry = geometry.representative_point()\n",
    "\n",
    "        # convert date string to YYYY-MM-DD\n",
    "        date_object = datetime.strptime(composite_start_date, \"%Y%m%d\")\n",
    "        dataspace_composite_start = date_object.strftime(\"%Y-%m-%d\")\n",
    "        date_object = datetime.strptime(composite_end_date, \"%Y%m%d\")\n",
    "        dataspace_composite_end = date_object.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        try:\n",
    "            dataspace_composite_products_all = queries_and_downloads.query_dataspace_by_polygon(\n",
    "                max_cloud_cover=cloud_cover,\n",
    "                start_date=dataspace_composite_start,\n",
    "                end_date=dataspace_composite_end,\n",
    "                area_of_interest=geometry,\n",
    "                max_records=100,\n",
    "                log=tile_log\n",
    "            )\n",
    "        except Exception as error:\n",
    "            tile_log.error(f\"query_dataspace_by_polygon received this error: {error}\")\n",
    "\n",
    "        titles = dataspace_composite_products_all[\"title\"].tolist()\n",
    "        sizes = list()\n",
    "        uuids = list()\n",
    "        for elem in dataspace_composite_products_all.itertuples(index=False):\n",
    "            sizes.append(elem[-2][\"download\"][\"size\"])\n",
    "            uuids.append(elem[-2][\"download\"][\"url\"].split(\"/\")[-1])\n",
    "\n",
    "        relative_orbit_numbers = dataspace_composite_products_all[\"relativeOrbitNumber\"].tolist()\n",
    "        processing_levels = dataspace_composite_products_all[\"processingLevel\"].tolist()\n",
    "        transformed_levels = ['Level-1C' if level == 'S2MSI1C' else 'Level-2A' for level in processing_levels]\n",
    "        cloud_covers = dataspace_composite_products_all[\"cloudCover\"].tolist()\n",
    "        begin_positions = dataspace_composite_products_all[\"startDate\"].tolist()\n",
    "        statuses = dataspace_composite_products_all[\"status\"].tolist()\n",
    "\n",
    "        scihub_compatible_df = pd.DataFrame({\"title\": titles,\n",
    "                                            \"size\": sizes,\n",
    "                                            \"beginposition\": begin_positions,\n",
    "                                            \"relativeorbitnumber\": relative_orbit_numbers,\n",
    "                                            \"cloudcoverpercentage\": cloud_covers,\n",
    "                                            \"processinglevel\": transformed_levels,\n",
    "                                            \"uuid\": uuids,\n",
    "                                            \"status\": statuses})\n",
    "\n",
    "        # check granule sizes on the server\n",
    "        scihub_compatible_df[\"size\"] = scihub_compatible_df[\"size\"].apply(lambda x: round(float(x) * 1e-6, 2))\n",
    "        # reassign to match the scihub variable\n",
    "        df_all = scihub_compatible_df\n",
    "\n",
    "\n",
    "    if download_source == \"scihub\":\n",
    "\n",
    "        try:\n",
    "            composite_products_all = queries_and_downloads.check_for_s2_data_by_date(\n",
    "                config_dict[\"tile_dir\"],\n",
    "                composite_start_date,\n",
    "                composite_end_date,\n",
    "                conf=credentials_dict,\n",
    "                cloud_cover=cloud_cover,\n",
    "                tile_id=tile_to_process,\n",
    "                producttype=None,\n",
    "            )\n",
    "\n",
    "        except Exception as error:\n",
    "            tile_log.error(\n",
    "                f\"check_for_s2_data_by_date failed, got this error :  {error}\"\n",
    "            )\n",
    "\n",
    "        tile_log.info(\n",
    "            \"--> Found {} L1C and L2A products for the composite:\".format(\n",
    "                len(composite_products_all)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        df_all = pd.DataFrame.from_dict(composite_products_all, orient=\"index\")\n",
    "\n",
    "        # check granule sizes on the server\n",
    "        df_all[\"size\"] = (\n",
    "            df_all[\"size\"]\n",
    "            .str.split(\" \")\n",
    "            .apply(lambda x: float(x[0]) * {\"GB\": 1e3, \"MB\": 1, \"KB\": 1e-3}[x[1]])\n",
    "        )\n",
    "\n",
    "    if download_source == \"scihub\":\n",
    "        min_granule_size = faulty_granule_threshold\n",
    "    else:\n",
    "        min_granule_size = 0  # Required for dataspace API which doesn't report size correctly (often reported as zero)\n",
    "\n",
    "    df = df_all.query(\"size >= \" + str(min_granule_size))\n",
    "\n",
    "    tile_log.info(\n",
    "        \"Removed {} faulty scenes <{}MB in size from the list\".format(\n",
    "            len(df_all) - len(df), min_granule_size\n",
    "        )\n",
    "    )\n",
    "    # find < threshold sizes, report to log\n",
    "    df_faulty = df_all.query(\"size < \" + str(min_granule_size))\n",
    "    for r in range(len(df_faulty)):\n",
    "        tile_log.info(\n",
    "            \"   {} MB: {}\".format(\n",
    "                df_faulty.iloc[r, :][\"size\"], df_faulty.iloc[r, :][\"title\"]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    l1c_products = df[df.processinglevel == \"Level-1C\"]\n",
    "    l2a_products = df[df.processinglevel == \"Level-2A\"]\n",
    "    tile_log.info(\"    {} L1C products\".format(l1c_products.shape[0]))\n",
    "    tile_log.info(\"    {} L2A products\".format(l2a_products.shape[0]))\n",
    "\n",
    "\n",
    "    rel_orbits = np.unique(l1c_products[\"relativeorbitnumber\"])\n",
    "    if len(rel_orbits) > 0:\n",
    "        if l1c_products.shape[0] > max_image_number / len(rel_orbits):\n",
    "            tile_log.info(\n",
    "                \"Capping the number of L1C products to {}\".format(max_image_number)\n",
    "            )\n",
    "            tile_log.info(\n",
    "                \"Relative orbits found covering tile: {}\".format(rel_orbits)\n",
    "            )\n",
    "            tile_log.info(\"dataspace branch reaches here\")\n",
    "            uuids = []\n",
    "            for orb in rel_orbits:\n",
    "                uuids = uuids + list(\n",
    "                    l1c_products.loc[\n",
    "                        l1c_products[\"relativeorbitnumber\"] == orb\n",
    "                    ].sort_values(by=[\"cloudcoverpercentage\"], ascending=True)[\n",
    "                        \"uuid\"\n",
    "                    ][\n",
    "                        : int(max_image_number / len(rel_orbits))\n",
    "                    ]\n",
    "                )\n",
    "            # keeps least cloudy n (max image number)\n",
    "            l1c_products = l1c_products[l1c_products[\"uuid\"].isin(uuids)]\n",
    "            tile_log.info(\n",
    "                \"    {} L1C products remain:\".format(l1c_products.shape[0])\n",
    "            )\n",
    "            for product in l1c_products[\"title\"]:\n",
    "                tile_log.info(\"       {}\".format(product))\n",
    "            tile_log.info(f\"len of L1C products for dataspace is {len(l1c_products['title'])}\")\n",
    "\n",
    "    rel_orbits = np.unique(l2a_products[\"relativeorbitnumber\"])\n",
    "    if len(rel_orbits) > 0:\n",
    "        if l2a_products.shape[0] > max_image_number / len(rel_orbits):\n",
    "            tile_log.info(\n",
    "                \"Capping the number of L2A products to {}\".format(max_image_number)\n",
    "            )\n",
    "            tile_log.info(\n",
    "                \"Relative orbits found covering tile: {}\".format(rel_orbits)\n",
    "            )\n",
    "            uuids = []\n",
    "            for orb in rel_orbits:\n",
    "                uuids = uuids + list(\n",
    "                    l2a_products.loc[\n",
    "                        l2a_products[\"relativeorbitnumber\"] == orb\n",
    "                    ].sort_values(by=[\"cloudcoverpercentage\"], ascending=True)[\n",
    "                        \"uuid\"\n",
    "                    ][\n",
    "                        : int(max_image_number / len(rel_orbits))\n",
    "                    ]\n",
    "                )\n",
    "            l2a_products = l2a_products[l2a_products[\"uuid\"].isin(uuids)]\n",
    "            tile_log.info(\n",
    "                \"    {} L2A products remain:\".format(l2a_products.shape[0])\n",
    "            )\n",
    "            for product in l2a_products[\"title\"]:\n",
    "                tile_log.info(\"       {}\".format(product))\n",
    "            tile_log.info(f\"len of L2A products for dataspace is {len(l2a_products['title'])}\")\n",
    "\n",
    "    if l1c_products.shape[0] > 0 and l2a_products.shape[0] > 0:\n",
    "        tile_log.info(\n",
    "            \"Filtering out L1C products that have the same 'beginposition' time stamp as an existing L2A product.\"\n",
    "        )\n",
    "        if download_source == \"scihub\":\n",
    "            (l1c_products,l2a_products,) = queries_and_downloads.filter_unique_l1c_and_l2a_data(df,log=tile_log)\n",
    "\n",
    "        if download_source == \"dataspace\":\n",
    "            l1c_products = queries_and_downloads.filter_unique_dataspace_products(l1c_products=l1c_products, l2a_products=l2a_products, log=tile_log)\n",
    "\n",
    "    df = None\n",
    "    tile_log.info(f\" {len(l1c_products['title'])} L1C products for the Composite\")\n",
    "    tile_log.info(f\" {len(l2a_products['title'])} L2A products for the Composite\")\n",
    "    \n",
    "    tile_log.info(\"Successfully queried the L1C and L2A products for the Composite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91dcc87f",
   "metadata": {},
   "source": [
    "### <a id='toc3_2_1_'></a>[Search for L2A Images Corresponding to L1C](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c90415",
   "metadata": {},
   "source": [
    "- The cell below searches our download directory for any existing imagery. If we have downloaded any imagery already, `pyeo` will remove the matching image from our search query.  \n",
    "\n",
    "- Secondly, if we have opted to use `scihub` as our `download_source`, then `pyeo` searches the Copernicus archive for any corresponding `L2A` products. If it finds a matching L2A product, then it removes the `L1C` counterpart from the query. The `dataspace` option handles this on the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98d740d4-f9f3-499b-aef7-e3f8bd4db8c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if config_dict[\"build_composite\"] or config_dict[\"do_all\"]:\n",
    "    # Search the local directories, composite/L2A and L1C, checking if scenes have already been downloaded and/or processed whilst checking their dir sizes\n",
    "    if download_source == \"scihub\":\n",
    "        if l1c_products.shape[0] > 0:\n",
    "            tile_log.info(\n",
    "                \"Checking for already downloaded and zipped L1C or L2A products and\"\n",
    "            )\n",
    "            tile_log.info(\"  availability of matching L2A products for download.\")\n",
    "            n = len(l1c_products)\n",
    "            drop = []\n",
    "            add = []\n",
    "            for r in range(n):\n",
    "                id = l1c_products.iloc[r, :][\"title\"]\n",
    "                search_term = (\n",
    "                    id.split(\"_\")[2]\n",
    "                    + \"_\"\n",
    "                    + id.split(\"_\")[3]\n",
    "                    + \"_\"\n",
    "                    + id.split(\"_\")[4]\n",
    "                    + \"_\"\n",
    "                    + id.split(\"_\")[5]\n",
    "                )\n",
    "                tile_log.info(\n",
    "                    \"Searching locally for file names containing: {}.\".format(\n",
    "                        search_term\n",
    "                    )\n",
    "                )\n",
    "                file_list = (\n",
    "                    [\n",
    "                        os.path.join(composite_l1_image_dir, f)\n",
    "                        for f in os.listdir(composite_l1_image_dir)\n",
    "                    ]\n",
    "                    + [\n",
    "                        os.path.join(composite_l2_image_dir, f)\n",
    "                        for f in os.listdir(composite_l2_image_dir)\n",
    "                    ]\n",
    "                    + [\n",
    "                        os.path.join(composite_l2_masked_image_dir, f)\n",
    "                        for f in os.listdir(composite_l2_masked_image_dir)\n",
    "                    ]\n",
    "                )\n",
    "                for f in file_list:\n",
    "                    if search_term in f:\n",
    "                        tile_log.info(\"  Product already downloaded: {}\".format(f))\n",
    "                        drop.append(l1c_products.index[r])\n",
    "                search_term = (\n",
    "                    \"*\"\n",
    "                    + id.split(\"_\")[2]\n",
    "                    + \"_\"\n",
    "                    + id.split(\"_\")[3]\n",
    "                    + \"_\"\n",
    "                    + id.split(\"_\")[4]\n",
    "                    + \"_\"\n",
    "                    + id.split(\"_\")[5]\n",
    "                    + \"*\"\n",
    "                )\n",
    "\n",
    "\n",
    "                tile_log.info(\n",
    "                    \"Searching on the data hub for files containing: {}.\".format(\n",
    "                        search_term\n",
    "                    )\n",
    "                )\n",
    "                matching_l2a_products = queries_and_downloads._file_api_query(\n",
    "                    user=sen_user,\n",
    "                    passwd=sen_pass,\n",
    "                    start_date=composite_start_date,\n",
    "                    end_date=composite_end_date,\n",
    "                    filename=search_term,\n",
    "                    cloud=cloud_cover,\n",
    "                    producttype=\"S2MSI2A\",\n",
    "                )\n",
    "\n",
    "                matching_l2a_products_df = pd.DataFrame.from_dict(\n",
    "                    matching_l2a_products, orient=\"index\"\n",
    "                )\n",
    "                # 07/03/2023: Matt - Applied Ali's fix for converting product size to MB to compare against faulty_grandule_threshold\n",
    "                if (\n",
    "                    len(matching_l2a_products_df) == 1\n",
    "                    and [\n",
    "                        float(x[0]) * {\"GB\": 1e3, \"MB\": 1, \"KB\": 1e-3}[x[1]]\n",
    "                        for x in [matching_l2a_products_df[\"size\"][0].split(\" \")]\n",
    "                    ][0]\n",
    "                    > faulty_granule_threshold\n",
    "                ):\n",
    "                    tile_log.info(\"Replacing L1C {} with L2A product:\".format(id))\n",
    "                    tile_log.info(\n",
    "                        \"              {}\".format(\n",
    "                            matching_l2a_products_df.iloc[0, :][\"title\"]\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                    drop.append(l1c_products.index[r])\n",
    "                    add.append(matching_l2a_products_df.iloc[0, :])\n",
    "                if len(matching_l2a_products_df) == 0:\n",
    "                    pass\n",
    "                if len(matching_l2a_products_df) > 1:\n",
    "                    # check granule sizes on the server\n",
    "                    matching_l2a_products_df[\"size\"] = (\n",
    "                        matching_l2a_products_df[\"size\"]\n",
    "                        .str.split(\" \")\n",
    "                        .apply(\n",
    "                            lambda x: float(x[0])\n",
    "                            * {\"GB\": 1e3, \"MB\": 1, \"KB\": 1e-3}[x[1]]\n",
    "                        )\n",
    "                    )\n",
    "                    matching_l2a_products_df = matching_l2a_products_df.query(\n",
    "                        \"size >= \" + str(faulty_granule_threshold)\n",
    "                    )\n",
    "                    if (\n",
    "                        matching_l2a_products_df.iloc[0, :][\"size\"]\n",
    "                        .str.split(\" \")\n",
    "                        .apply(\n",
    "                            lambda x: float(x[0])\n",
    "                            * {\"GB\": 1e3, \"MB\": 1, \"KB\": 1e-3}[x[1]]\n",
    "                        )\n",
    "                        > faulty_granule_threshold\n",
    "                    ):\n",
    "                        tile_log.info(\"Replacing L1C {} with L2A product:\".format(id))\n",
    "                        tile_log.info(\n",
    "                            \"              {}\".format(\n",
    "                                matching_l2a_products_df.iloc[0, :][\"title\"]\n",
    "                            )\n",
    "                        )\n",
    "                        drop.append(l1c_products.index[r])\n",
    "                        add.append(matching_l2a_products_df.iloc[0, :])\n",
    "            if len(drop) > 0:\n",
    "                l1c_products = l1c_products.drop(index=drop)\n",
    "            if len(add) > 0:\n",
    "                # l2a_products = l2a_products.append(add)\n",
    "                add = pd.DataFrame(add)\n",
    "                l2a_products = pd.concat([l2a_products, add])\n",
    "\n",
    "            tile_log.info(\"\\n Successfully searched for the L2A counterparts for the L1C products for the Composite\")\n",
    "        \n",
    "    # here, dataspace and scihub derived l1c_products and l2a_products lists are the \"same\"\n",
    "    l2a_products = l2a_products.drop_duplicates(subset=\"title\")\n",
    "    tile_log.info(\n",
    "        \"    {} L1C products remaining for download\".format(\n",
    "            l1c_products.shape[0]\n",
    "        )\n",
    "    )\n",
    "    tile_log.info(\n",
    "        \"    {} L2A products remaining for download\".format(\n",
    "            l2a_products.shape[0]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    tile_log.info(\"Cell successfully finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8108c91a",
   "metadata": {},
   "source": [
    "## <a id='toc3_3_'></a>[Download Sentinel-2 Composite Imagery](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e777708",
   "metadata": {},
   "source": [
    "### <a id='toc3_3_1_'></a>[Download and Process L1Cs](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef54e1d",
   "metadata": {},
   "source": [
    "- From the `log` output above in the previous section, we can see that `pyeo` has found a matching `L2A` image for each of the `L1Cs` in our search query. So now we have only L2As in our search query.  \n",
    "\n",
    "- If we did have `L1Cs` in our search query, then the cell below would download these L1Cs and apply `atmospheric_correction` using `Sen2Cor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0e2fa54-3ba0-4dd2-b1b2-7b5b7a50fccc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if config_dict[\"build_composite\"] or config_dict[\"do_all\"]:\n",
    "    if l1c_products.shape[0] > 0:\n",
    "        tile_log.info(f\"Downloading Sentinel-2 L1C products from {download_source}:\")\n",
    "\n",
    "        if download_source == \"scihub\":\n",
    "\n",
    "            queries_and_downloads.download_s2_data_from_df(\n",
    "                l1c_products,\n",
    "                composite_l1_image_dir,\n",
    "                composite_l2_image_dir,\n",
    "                source=\"scihub\",\n",
    "                user=sen_user,\n",
    "                passwd=sen_pass,\n",
    "                try_scihub_on_fail=True,\n",
    "            )\n",
    "\n",
    "        if download_source == \"dataspace\":\n",
    "\n",
    "            queries_and_downloads.download_s2_data_from_dataspace(\n",
    "                product_df=l1c_products,\n",
    "                l1c_directory=composite_l1_image_dir,\n",
    "                l2a_directory=composite_l2_image_dir,\n",
    "                dataspace_username=sen_user,\n",
    "                dataspace_password=sen_pass,\n",
    "                log=tile_log\n",
    "            )\n",
    "        '''\n",
    "        tile_log.info(\"Atmospheric correction with sen2cor.\")\n",
    "        raster_manipulation.atmospheric_correction(\n",
    "            composite_l1_image_dir,\n",
    "            composite_l2_image_dir,\n",
    "            sen2cor_path,\n",
    "            delete_unprocessed_image=False,\n",
    "            log=tile_log,\n",
    "        )\n",
    "        '''\n",
    "    tile_log.info(\"Successfully downloaded the Sentinel-2 L1C products\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d51568",
   "metadata": {},
   "source": [
    "### <a id='toc3_3_2_'></a>[Download L2As](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54385c8f",
   "metadata": {},
   "source": [
    "In this subsection, we will download the L2As from our search query.  \n",
    "\n",
    "But first, let's take a look at what our search query result, `l2a_products` looks like by printing the first 3 rows with `.head(3)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a9465c05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if config_dict[\"build_composite\"] or config_dict[\"do_all\"]:\n",
    "    l2a_products.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e755487d",
   "metadata": {},
   "source": [
    "Let's highlight a few columns of interest:  \n",
    "\n",
    "In the cell output above, we can see the product `uuid` as the dataframe index (*the first column, it has no column name*). These are the unique identifiers used to distinguish the scenes from each other.  \n",
    "\n",
    "From the `title` column, we can see the titles of each product, the titles themselves show us important information, for example: the Satellite (*S2A or S2B*), the Sensor (*MSI*), the product type (*L2A*), the date the image was captured (*YYYYMMDD*) or the corresponding tile for the image (*TXXXXX*).\n",
    "\n",
    "We can also see if the product is online or in the Long-Term Archive (`LTA`), by looking at the column `ondemand`, where `false` indicates the product is in the LTA or `true` indicates the product is online and ready for download."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372aefa6",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee158dd",
   "metadata": {},
   "source": [
    "Now, let's download the `L2As` in our search query `l2a_products`, by asking `pyeo` to download these images from the Copernicus archive. If any incomplete downloads are present from a previous run (*remember, pyeo is an iterative download, classification and change detection process*), then `pyeo` will flag these files to the user through the log file.\n",
    "\n",
    "If the images are in the Long Term Archive (`LTA`), then `pyeo` will linearly activate and wait for the LTA image to become available, before downloading and moving onto the next L2A in the search query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25dce346",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if config_dict[\"build_composite\"] or config_dict[\"do_all\"]:\n",
    "    if l2a_products.shape[0] > 0:\n",
    "        tile_log.info(\"Downloading Sentinel-2 L2A products.\")\n",
    "\n",
    "        if download_source == \"scihub\":\n",
    "\n",
    "            queries_and_downloads.download_s2_data(\n",
    "                l2a_products.to_dict(\"index\"),\n",
    "                composite_l1_image_dir,\n",
    "                composite_l2_image_dir,\n",
    "                source=\"scihub\",\n",
    "                user=sen_user,\n",
    "                passwd=sen_pass,\n",
    "                try_scihub_on_fail=True,\n",
    "            )\n",
    "        if download_source == \"dataspace\":\n",
    "\n",
    "            queries_and_downloads.download_s2_data_from_dataspace(\n",
    "                product_df=l2a_products,\n",
    "                l1c_directory=composite_l1_image_dir,\n",
    "                l2a_directory=composite_l2_image_dir,\n",
    "                dataspace_username=sen_user,\n",
    "                dataspace_password=sen_pass,\n",
    "                log=tile_log\n",
    "            )\n",
    "\n",
    "    # check for incomplete L2A downloads\n",
    "    incomplete_downloads, sizes = raster_manipulation.find_small_safe_dirs(\n",
    "        composite_l2_image_dir, threshold=faulty_granule_threshold * 1024 * 1024\n",
    "    )\n",
    "    if len(incomplete_downloads) > 0:\n",
    "        for index, safe_dir in enumerate(incomplete_downloads):\n",
    "            if sizes[\n",
    "                index\n",
    "            ] / 1024 / 1024 < faulty_granule_threshold and os.path.exists(safe_dir):\n",
    "                tile_log.warning(\"Found likely incomplete download of size {} MB: {}\".format(\n",
    "                        str(round(sizes[index] / 1024 / 1024)), safe_dir))\n",
    "\n",
    "    tile_log.info(\"---------------------------------------------------------------\")\n",
    "    tile_log.info(\"Image download and atmospheric correction for composite is complete.\")\n",
    "    tile_log.info(\"---------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c2e4e7",
   "metadata": {},
   "source": [
    "### <a id='toc3_3_3_'></a>[Housekeeping](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699c153a",
   "metadata": {},
   "source": [
    "The cell below performs some housekeeping if we have told `pyeo` to delete or zip imagery. This functionality is useful for ensuring disk space is kept to a minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "664b70df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if config_dict[\"build_composite\"] or config_dict[\"do_all\"]:\n",
    "    if config_dict[\"do_delete\"]:\n",
    "        tile_log.info(\"---------------------------------------------------------------\")\n",
    "        tile_log.info(\"Deleting downloaded L1C images for composite, keeping only derived L2A products\")\n",
    "        tile_log.info(\n",
    "            \"---------------------------------------------------------------\"\n",
    "        )\n",
    "        directory = composite_l1_image_dir\n",
    "        tile_log.info(\"Deleting {}\".format(directory))\n",
    "        shutil.rmtree(directory)\n",
    "        tile_log.info(\n",
    "            \"---------------------------------------------------------------\"\n",
    "        )\n",
    "        tile_log.info(\"Deletion of L1C images complete. Keeping only L2A images.\")\n",
    "        tile_log.info(\n",
    "            \"---------------------------------------------------------------\"\n",
    "        )\n",
    "    else:\n",
    "        if config_dict[\"do_zip\"]:\n",
    "            tile_log.info(\"---------------------------------------------------------------\")\n",
    "            tile_log.info(\"Zipping downloaded L1C images for composite after atmospheric correction\")\n",
    "            tile_log.info(\"---------------------------------------------------------------\")\n",
    "            filesystem_utilities.zip_contents(composite_l1_image_dir)\n",
    "            tile_log.info(\"---------------------------------------------------------------\")\n",
    "            tile_log.info(\"Zipping complete\")\n",
    "            tile_log.info(\"---------------------------------------------------------------\")\n",
    "\n",
    "    tile_log.info(\"Cell successfully finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc20c04f",
   "metadata": {},
   "source": [
    "## <a id='toc3_4_'></a>[Process the Downloaded Imagery](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2363aa7c",
   "metadata": {},
   "source": [
    "Now that we have downloaded the L2A Imagery, we will process the imagery. Processing refers to:  \n",
    "\n",
    "1. Applying the `SCL Cloud Mask` to remove cloud, haze or cloud shadow pixels from the imagery.\n",
    "2. Applying a `Processing Baseline Correction Offset` to the imagery, if applicable.\n",
    "3. Create `Quicklooks` (*.png*) of the processed imagery."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86356816-41ab-4e2b-af88-21e5d08c250d",
   "metadata": {},
   "source": [
    "### Check for pre-downloaded Imagery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2c3f23d5-26f0-4079-8b70-d1f9063d0520",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if config_dict[\"build_composite\"] or config_dict[\"do_all\"]:\n",
    "    tile_log.info(\"---------------------------------------------------------------\")\n",
    "    tile_log.info(\"Applying simple cloud, cloud shadow and haze mask based on SCL files and stacking the masked band raster files.\")\n",
    "    tile_log.info(\"---------------------------------------------------------------\")\n",
    "\n",
    "    directory = composite_l2_masked_image_dir\n",
    "    masked_file_paths = [\n",
    "        f\n",
    "        for f in os.listdir(directory)\n",
    "        if f.endswith(\".tif\") and os.path.isfile(os.path.join(directory, f))\n",
    "    ]\n",
    "\n",
    "    directory = composite_l2_image_dir\n",
    "    l2a_zip_file_paths = [f for f in os.listdir(directory) if f.endswith(\".zip\")]\n",
    "\n",
    "    if len(l2a_zip_file_paths) > 0:\n",
    "        for f in l2a_zip_file_paths:\n",
    "            # check whether the zipped file has already been cloud masked\n",
    "            zip_timestamp = filesystem_utilities.get_image_acquisition_time(\n",
    "                os.path.basename(f)\n",
    "            ).strftime(\"%Y%m%dT%H%M%S\")\n",
    "            if any(zip_timestamp in f for f in masked_file_paths):\n",
    "                continue\n",
    "            else:\n",
    "                # extract it if not\n",
    "                filesystem_utilities.unzip_contents(\n",
    "                    os.path.join(composite_l2_image_dir, f),\n",
    "                    ifstartswith=\"S2\",\n",
    "                    ending=\".SAFE\",\n",
    "                )\n",
    "\n",
    "    directory = composite_l2_image_dir\n",
    "    l2a_safe_file_paths = [\n",
    "        f\n",
    "        for f in os.listdir(directory)\n",
    "        if f.endswith(\".SAFE\") and os.path.isdir(os.path.join(directory, f))\n",
    "    ]\n",
    "\n",
    "    files_for_cloud_masking = []\n",
    "    if len(l2a_safe_file_paths) > 0:\n",
    "        for f in l2a_safe_file_paths:\n",
    "            # check whether the L2A SAFE file has already been cloud masked\n",
    "            safe_timestamp = filesystem_utilities.get_image_acquisition_time(\n",
    "                os.path.basename(f)\n",
    "            ).strftime(\"%Y%m%dT%H%M%S\")\n",
    "            if any(safe_timestamp in f for f in masked_file_paths):\n",
    "                continue\n",
    "            else:\n",
    "                # add it to the list of files to do if it has not been cloud masked yet\n",
    "                files_for_cloud_masking = files_for_cloud_masking + [f]\n",
    "\n",
    "    tile_log.info(\"Cell successfully finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa852abf",
   "metadata": {},
   "source": [
    "### <a id='toc3_4_2_'></a>[Apply SCL Cloud Mask](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327f596f",
   "metadata": {},
   "source": [
    "Optical data is affected by the presence of clouds over the land cover of interest. So, we use `apply_scl_cloud_mask` to remove cloudy pixels from the imagery, as we are not interested in clouds.\n",
    "\n",
    "The cell below peforms two things:\n",
    "\n",
    "- Checks whether any L2A SAFE files have been cloud masked from a previous run.\n",
    "\n",
    "- If any L2A SAFE files have not been cloud masked, then `apply_scl_cloud_mask` is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d05c32f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if config_dict[\"build_composite\"] or config_dict[\"do_all\"]:\n",
    "    if len(files_for_cloud_masking) == 0:\n",
    "        tile_log.info(\"No L2A images found for cloud masking. They may already have been done.\")\n",
    "    else:\n",
    "        raster_manipulation.apply_scl_cloud_mask(\n",
    "            composite_l2_image_dir,\n",
    "            composite_l2_masked_image_dir,\n",
    "            scl_classes=[0, 1, 2, 3, 8, 9, 10, 11],\n",
    "            buffer_size=buffer_size_composite,\n",
    "            bands=bands,\n",
    "            out_resolution=out_resolution,\n",
    "            haze=None,\n",
    "            epsg=epsg,\n",
    "            skip_existing=skip_existing,)\n",
    "\n",
    "    tile_log.info(\"Successfully applied the Cloud Mask\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4819b1b",
   "metadata": {},
   "source": [
    "### <a id='toc3_4_3_'></a>[Apply Processing Baseline Offset](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701b54b2",
   "metadata": {},
   "source": [
    "Before Sentinel-2 imagery is provided to the user as L1C or L2A formats, the raw imagery (L0) are processed by the ESA Copernicus Ground Segment ([see here](https://sentinel.esa.int/web/sentinel/user-guides/sentinel-2-msi/processing-levels)). The algorithms used in the processing baseline, are indicated by the field `N0XXX` in the product title and the changes introduced by each processing baseline iteration are listed [here](https://sentinel.esa.int/web/sentinel/technical-guides/sentinel-2-msi/processing-baseline).\n",
    "\n",
    "The advent of processing baseline `N0400` introduced an offset of `-1000` in the spectral reflectance values, the reasoning and suggested reading can be viewed [here](https://forum.step.esa.int/t/info-introduction-of-additional-radiometric-offset-in-pb04-00-products/35431). Therefore, to ensure that the spectral reflectance of imagery before and after `N0400` can be compared, we apply the offset correction of `+1000`.\n",
    "\n",
    "The cell below, applies such an offset correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6dc9e3a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if config_dict[\"build_composite\"] or config_dict[\"do_all\"]:\n",
    "    tile_log.info(\"---------------------------------------------------------------\")\n",
    "    tile_log.info(\"Offsetting cloud masked L2A images for composite.\")\n",
    "    tile_log.info(\"---------------------------------------------------------------\")\n",
    "\n",
    "    raster_manipulation.apply_processing_baseline_offset_correction_to_tiff_file_directory(\n",
    "        composite_l2_masked_image_dir, composite_l2_masked_image_dir)\n",
    "\n",
    "    tile_log.info(\"---------------------------------------------------------------\")\n",
    "    tile_log.info(\"Offsetting of cloud masked L2A images for composite complete.\")\n",
    "    tile_log.info(\"---------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f63cf8e",
   "metadata": {},
   "source": [
    "### <a id='toc3_4_4_'></a>[Create Quicklooks of Cloud-Masked Images](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6febea00",
   "metadata": {},
   "source": [
    "- We can also create quicklooks of the Cloud-Masked images. These are especially useful for viewing the images quickly using a standard photo viewer, and for use in presentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8a54841d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if config_dict[\"build_composite\"] or config_dict[\"do_all\"]:\n",
    "    if config_dict[\"do_quicklooks\"] or config_dict[\"do_all\"]:\n",
    "        tile_log.info(\n",
    "            \"---------------------------------------------------------------\"\n",
    "        )\n",
    "        tile_log.info(\"Producing quicklooks.\")\n",
    "        tile_log.info(\n",
    "            \"---------------------------------------------------------------\"\n",
    "        )\n",
    "        dirs_for_quicklooks = [composite_l2_masked_image_dir]\n",
    "        for main_dir in dirs_for_quicklooks:\n",
    "            files = [\n",
    "                f.path\n",
    "                for f in os.scandir(main_dir)\n",
    "                if f.is_file() and os.path.basename(f).endswith(\".tif\")\n",
    "            ]\n",
    "            # files = [ f.path for f in os.scandir(main_dir) if f.is_file() and os.path.basename(f).endswith(\".tif\") and \"class\" in os.path.basename(f) ] # do classification images only\n",
    "            if len(files) == 0:\n",
    "                tile_log.warning(\"No images found in {}.\".format(main_dir))\n",
    "            else:\n",
    "                for f in files:\n",
    "                    quicklook_path = os.path.join(\n",
    "                        quicklook_dir,\n",
    "                        os.path.basename(f).split(\".\")[0] + \".png\",\n",
    "                    )\n",
    "                    tile_log.info(\"Creating quicklook: {}\".format(quicklook_path))\n",
    "                    raster_manipulation.create_quicklook(\n",
    "                        f,\n",
    "                        quicklook_path,\n",
    "                        width=512,\n",
    "                        height=512,\n",
    "                        format=\"PNG\",\n",
    "                        bands=[3, 2, 1],\n",
    "                        scale_factors=[[0, 2000, 0, 255]],\n",
    "                    )\n",
    "        tile_log.info(\"Quicklooks complete.\")\n",
    "    else:\n",
    "        tile_log.info(\"Quicklook option disabled in ini file.\")\n",
    "\n",
    "\n",
    "    if config_dict[\"do_zip\"]:\n",
    "        tile_log.info(\n",
    "            \"---------------------------------------------------------------\"\n",
    "        )\n",
    "        tile_log.info(\n",
    "            \"Zipping downloaded L2A images for composite after cloud masking and band stacking\"\n",
    "        )\n",
    "        tile_log.info(\n",
    "            \"---------------------------------------------------------------\"\n",
    "        )\n",
    "        filesystem_utilities.zip_contents(composite_l2_image_dir)\n",
    "        tile_log.info(\n",
    "            \"---------------------------------------------------------------\"\n",
    "        )\n",
    "        tile_log.info(\"Zipping complete\")\n",
    "        tile_log.info(\n",
    "            \"---------------------------------------------------------------\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23034122",
   "metadata": {},
   "source": [
    "## <a id='toc3_5_'></a>[Create Composite from the Baseline Imagery](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a428cac1",
   "metadata": {},
   "source": [
    "Now we come to the last section of Tutorial Section 2. Previously, we have queried the Copernicus archive for Sentinel-2 images that matched our search criteria, we evaluated which L2A products were present in the archive to avoid unecessary processing from pyeo for conversion from L1C to L2A. We then downloaded the resulting imagery, applied a cloud mask and a baseline offset correction, if necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c969c686",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "if config_dict[\"build_composite\"] or config_dict[\"do_all\"]:\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "        tile_log.info(\"---------------------------------------------------------------\")\n",
    "        tile_log.info(\n",
    "            \"Building initial cloud-free median composite from directory {}\".format(\n",
    "                composite_l2_masked_image_dir\n",
    "            )\n",
    "        )\n",
    "        tile_log.info(\"---------------------------------------------------------------\")\n",
    "        directory = composite_l2_masked_image_dir\n",
    "        masked_file_paths = [\n",
    "            f\n",
    "            for f in os.listdir(directory)\n",
    "            if f.endswith(\".tif\") and os.path.isfile(os.path.join(directory, f))\n",
    "        ]\n",
    "\n",
    "        if len(masked_file_paths) > 0:\n",
    "            raster_manipulation.clever_composite_directory(\n",
    "                composite_l2_masked_image_dir,\n",
    "                composite_dir,\n",
    "                chunks=config_dict[\"chunks\"],\n",
    "                generate_date_images=True,\n",
    "                missing_data_value=0,\n",
    "            )\n",
    "            tile_log.info(\"---------------------------------------------------------------\")\n",
    "            tile_log.info(\"Baseline composite complete.\")\n",
    "            tile_log.info(\"---------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2762c54",
   "metadata": {},
   "source": [
    "### <a id='toc3_5_1_'></a>[Create Quicklook of the Composite](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a2321632",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if config_dict[\"build_composite\"] or config_dict[\"do_all\"]:\n",
    "    if config_dict[\"do_quicklooks\"] or config_dict[\"do_all\"]:\n",
    "        tile_log.info(\"---------------------------------------------------------------\")\n",
    "        tile_log.info(\"Producing quicklooks.\")\n",
    "        tile_log.info(\"---------------------------------------------------------------\")\n",
    "        dirs_for_quicklooks = [composite_dir]\n",
    "        for main_dir in dirs_for_quicklooks:\n",
    "            files = [\n",
    "                f.path\n",
    "                for f in os.scandir(main_dir)\n",
    "                if f.is_file() and os.path.basename(f).endswith(\".tif\")\n",
    "            ]\n",
    "            if len(files) == 0:\n",
    "                tile_log.warning(\"No images found in {}.\".format(main_dir))\n",
    "            else:\n",
    "                for f in files:\n",
    "                    quicklook_path = os.path.join(\n",
    "                        quicklook_dir,\n",
    "                        os.path.basename(f).split(\".\")[0] + \".png\",\n",
    "                    )\n",
    "                    tile_log.info(\n",
    "                        \"Creating quicklook: {}\".format(quicklook_path)\n",
    "                    )\n",
    "                    raster_manipulation.create_quicklook(\n",
    "                        f,\n",
    "                        quicklook_path,\n",
    "                        width=512,\n",
    "                        height=512,\n",
    "                        format=\"PNG\",\n",
    "                        bands=[3, 2, 1],\n",
    "                        scale_factors=[[0, 2000, 0, 255]],\n",
    "                    )\n",
    "        tile_log.info(\"Quicklooks complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc25d1c",
   "metadata": {},
   "source": [
    "### <a id='toc3_5_2_'></a>[Final Housekeeping](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f2c3c7",
   "metadata": {},
   "source": [
    "Now that we have created our composite and produced any quicklooks, we tell `pyeo` to delete or compress the cloud-masked L2A images that the composite was derived from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3200fcba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if config_dict[\"build_composite\"] or config_dict[\"do_all\"]:\n",
    "    if config_dict[\"do_quicklooks\"] or config_dict[\"do_all\"]:\n",
    "        if config_dict[\"do_delete\"]:\n",
    "            tile_log.info(\n",
    "                \"---------------------------------------------------------------\"\n",
    "            )\n",
    "            tile_log.info(\n",
    "                \"Deleting intermediate cloud-masked L2A images used for the baseline composite\"\n",
    "            )\n",
    "            tile_log.info(\n",
    "                \"---------------------------------------------------------------\"\n",
    "            )\n",
    "            f = composite_l2_masked_image_dir\n",
    "            tile_log.info(\"Deleting {}\".format(f))\n",
    "            shutil.rmtree(f)\n",
    "            tile_log.info(\n",
    "                \"---------------------------------------------------------------\"\n",
    "            )\n",
    "            tile_log.info(\"Intermediate file products have been deleted.\")\n",
    "            tile_log.info(\"They can be reprocessed from the downloaded L2A images.\")\n",
    "            tile_log.info(\n",
    "                \"---------------------------------------------------------------\"\n",
    "            )\n",
    "        else:\n",
    "            if config_dict[\"do_zip\"]:\n",
    "                tile_log.info(\n",
    "                    \"---------------------------------------------------------------\"\n",
    "                )\n",
    "                tile_log.info(\n",
    "                    \"Zipping cloud-masked L2A images used for the baseline composite\"\n",
    "                )\n",
    "                tile_log.info(\n",
    "                    \"---------------------------------------------------------------\"\n",
    "                )\n",
    "                filesystem_utilities.zip_contents(composite_l2_masked_image_dir)\n",
    "                tile_log.info(\n",
    "                    \"---------------------------------------------------------------\"\n",
    "                )\n",
    "                tile_log.info(\"Zipping complete\")\n",
    "                tile_log.info(\n",
    "                    \"---------------------------------------------------------------\"\n",
    "                )\n",
    "\n",
    "        tile_log.info(\n",
    "            \"---------------------------------------------------------------\"\n",
    "        )\n",
    "        tile_log.info(\n",
    "            \"Compressing tiff files in directory {} and all subdirectories\".format(\n",
    "                composite_dir\n",
    "            )\n",
    "        )\n",
    "        tile_log.info(\n",
    "            \"---------------------------------------------------------------\"\n",
    "        )\n",
    "        for root, dirs, files in os.walk(composite_dir):\n",
    "            all_tiffs = [\n",
    "                image_name for image_name in files if image_name.endswith(\".tif\")\n",
    "            ]\n",
    "            for this_tiff in all_tiffs:\n",
    "                raster_manipulation.compress_tiff(\n",
    "                    os.path.join(root, this_tiff), os.path.join(root, this_tiff)\n",
    "                )\n",
    "\n",
    "        tile_log.info(\n",
    "            \"---------------------------------------------------------------\"\n",
    "        )\n",
    "        tile_log.info(\n",
    "            \"Baseline image composite, file compression, zipping and deletion of\"\n",
    "        )\n",
    "        tile_log.info(\"intermediate file products (if selected) are complete.\")\n",
    "        tile_log.info(\n",
    "            \"---------------------------------------------------------------\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c074151b",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[Session 3: Change Detection](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d62a50b",
   "metadata": {},
   "source": [
    "- If we are returning to this notebook after a break, we need to re-run the cells in the subsection - [Directory and Variable Setup.](#toc3_1_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383eb810",
   "metadata": {},
   "source": [
    "## <a id='toc4_3_'></a>[Query Sentinel-2 Change Imagery](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3e11d368",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if config_dict[\"do_all\"] or config_dict[\"do_download\"]:\n",
    "    tile_log.info(\"---------------------------------------------------------------\")\n",
    "    tile_log.info(\n",
    "        \"Downloading change detection images between {} and {} with cloud cover <= {}\".format(\n",
    "            start_date, end_date, cloud_cover\n",
    "        )\n",
    "    )\n",
    "    tile_log.info(\"---------------------------------------------------------------\")\n",
    "    if download_source == \"dataspace\":\n",
    "\n",
    "        try:\n",
    "            tiles_geom_path = os.path.join(config_dict[\"pyeo_dir\"], os.path.join(config_dict[\"geometry_dir\"], config_dict[\"s2_tiles_filename\"]))\n",
    "            tiles_geom = gpd.read_file(os.path.abspath(tiles_geom_path))\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            tile_log.error(f\"tiles_geom does not exist, the path is :{tiles_geom_path}\")\n",
    "\n",
    "        tile_geom = tiles_geom[tiles_geom[\"Name\"] == tile_to_process]\n",
    "        tile_geom = tile_geom.to_crs(epsg=4326)\n",
    "        geometry = tile_geom[\"geometry\"].iloc[0]\n",
    "        geometry = geometry.representative_point()\n",
    "\n",
    "        # convert date string to YYYY-MM-DD\n",
    "        date_object = datetime.strptime(start_date, \"%Y%m%d\")\n",
    "        dataspace_change_start = date_object.strftime(\"%Y-%m-%d\")\n",
    "        date_object = datetime.strptime(end_date, \"%Y%m%d\")\n",
    "        dataspace_change_end = date_object.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        try:\n",
    "            dataspace_change_products_all = queries_and_downloads.query_dataspace_by_polygon(\n",
    "                max_cloud_cover=cloud_cover,\n",
    "                start_date=dataspace_change_start,\n",
    "                end_date=dataspace_change_end,\n",
    "                area_of_interest=geometry,\n",
    "                max_records=100,\n",
    "                log=tile_log\n",
    "            )\n",
    "        except Exception as error:\n",
    "            tile_log.error(f\"query_by_polygon received this error: {error}\")\n",
    "\n",
    "        titles = dataspace_change_products_all[\"title\"].tolist()\n",
    "        sizes = list()\n",
    "        uuids = list()\n",
    "        for elem in dataspace_change_products_all.itertuples(index=False):\n",
    "            sizes.append(elem[-2][\"download\"][\"size\"])\n",
    "            uuids.append(elem[-2][\"download\"][\"url\"].split(\"/\")[-1])\n",
    "\n",
    "\n",
    "        relative_orbit_numbers = dataspace_change_products_all[\"relativeOrbitNumber\"].tolist()\n",
    "        processing_levels = dataspace_change_products_all[\"processingLevel\"].tolist()\n",
    "        transformed_levels = ['Level-1C' if level == 'S2MSI1C' else 'Level-2A' for level in processing_levels]\n",
    "        cloud_covers = dataspace_change_products_all[\"cloudCover\"].tolist()\n",
    "        begin_positions = dataspace_change_products_all[\"startDate\"].tolist()\n",
    "        statuses = dataspace_change_products_all[\"status\"].tolist()\n",
    "\n",
    "        scihub_compatible_df = pd.DataFrame({\"title\": titles,\n",
    "                                            \"size\": sizes,\n",
    "                                            \"beginposition\": begin_positions,\n",
    "                                            \"relativeorbitnumber\": relative_orbit_numbers,\n",
    "                                            \"cloudcoverpercentage\": cloud_covers,\n",
    "                                            \"processinglevel\": transformed_levels,\n",
    "                                            \"uuid\": uuids,\n",
    "                                            \"status\": statuses})\n",
    "\n",
    "        # check granule sizes on the server\n",
    "        scihub_compatible_df[\"size\"] = scihub_compatible_df[\"size\"].apply(lambda x: round(float(x) * 1e-6, 2))\n",
    "        # reassign to match the scihub variable\n",
    "        df_all = scihub_compatible_df\n",
    "\n",
    "    if download_source == \"scihub\":\n",
    "        products_all = queries_and_downloads.check_for_s2_data_by_date(\n",
    "            config_dict[\"tile_dir\"],\n",
    "            start_date,\n",
    "            end_date,\n",
    "            credentials_dict,\n",
    "            cloud_cover=cloud_cover,\n",
    "            tile_id=tile_to_process,\n",
    "            producttype=None,  # \"S2MSI2A\" or \"S2MSI1C\"\n",
    "        )\n",
    "        tile_log.info(\n",
    "            \"--> Found {} L1C and L2A products for change detection:\".format(\n",
    "                len(products_all)\n",
    "            )\n",
    "        )\n",
    "        df_all = pd.DataFrame.from_dict(products_all, orient=\"index\")\n",
    "\n",
    "        # check granule sizes on the server\n",
    "        df_all[\"size\"] = (\n",
    "            df_all[\"size\"]\n",
    "            .str.split(\" \")\n",
    "            .apply(lambda x: float(x[0]) * {\"GB\": 1e3, \"MB\": 1, \"KB\": 1e-3}[x[1]])\n",
    "        )\n",
    "\n",
    "    # here the main call (from if download_source == \"scihub\" branch) is resumed\n",
    "    df = df_all.query(\"size >= \" + str(faulty_granule_threshold))\n",
    "    tile_log.info(\n",
    "        \"Removed {} faulty scenes <{}MB in size from the list:\".format(\n",
    "            len(df_all) - len(df), faulty_granule_threshold\n",
    "        )\n",
    "    )\n",
    "    df_faulty = df_all.query(\"size < \" + str(faulty_granule_threshold))\n",
    "    for r in range(len(df_faulty)):\n",
    "        tile_log.info(\n",
    "            \"   {} MB: {}\".format(\n",
    "                df_faulty.iloc[r, :][\"size\"], df_faulty.iloc[r, :][\"title\"]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    l1c_products = df[df.processinglevel == \"Level-1C\"]\n",
    "    l2a_products = df[df.processinglevel == \"Level-2A\"]\n",
    "    tile_log.info(\"    {} L1C products\".format(l1c_products.shape[0]))\n",
    "    tile_log.info(\"    {} L2A products\".format(l2a_products.shape[0]))\n",
    "\n",
    "    if l1c_products.shape[0] > 0 and l2a_products.shape[0] > 0:\n",
    "        tile_log.info(\n",
    "            \"Filtering out L1C products that have the same 'beginposition' time stamp as an existing L2A product.\"\n",
    "        )\n",
    "        if download_source == \"scihub\":\n",
    "            (l1c_products,l2a_products,) = queries_and_downloads.filter_unique_l1c_and_l2a_data(df,log=tile_log)\n",
    "\n",
    "        if download_source == \"dataspace\":\n",
    "            l1c_products = queries_and_downloads.filter_unique_dataspace_products(l1c_products=l1c_products, l2a_products=l2a_products, log=tile_log)\n",
    "\n",
    "        tile_log.info(\n",
    "            \"--> {} L1C and L2A products with unique 'beginposition' time stamp for the composite:\".format(\n",
    "                l1c_products.shape[0] + l2a_products.shape[0]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    df = None\n",
    "    tile_log.info(f\" {len(l1c_products['title'])} L1C Change Images\")\n",
    "    tile_log.info(f\" {len(l2a_products['title'])} L2A Change Images\")\n",
    "    \n",
    "    tile_log.info(\"Cell successfully finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989363f8-0ad7-4ac2-8c4a-1619abde3f4e",
   "metadata": {},
   "source": [
    "## Search for L2A Images Corresponding to L1C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4d243e7c-4db3-4547-9845-b212a8fd6e08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if config_dict[\"do_all\"] or config_dict[\"do_download\"]:\n",
    "    if download_source == \"scihub\":    \n",
    "        if l1c_products.shape[0] > 0:\n",
    "            tile_log.info(\"Checking for availability of L2A products to minimise download and atmospheric correction of L1C products.\")\n",
    "            n = len(l1c_products)\n",
    "            drop = []\n",
    "            add = []\n",
    "            for r in range(n):\n",
    "                id = l1c_products.iloc[r, :][\"title\"]\n",
    "                search_term = (\n",
    "                    \"*\"\n",
    "                    + id.split(\"_\")[2]\n",
    "                    + \"_\"\n",
    "                    + id.split(\"_\")[3]\n",
    "                    + \"_\"\n",
    "                    + id.split(\"_\")[4]\n",
    "                    + \"_\"\n",
    "                    + id.split(\"_\")[5]\n",
    "                    + \"*\"\n",
    "                )\n",
    "                tile_log.info(\"Search term: {}.\".format(search_term))\n",
    "                matching_l2a_products = queries_and_downloads._file_api_query(\n",
    "                    user=sen_user,\n",
    "                    passwd=sen_pass,\n",
    "                    start_date=start_date,\n",
    "                    end_date=end_date,\n",
    "                    filename=search_term,\n",
    "                    cloud=cloud_cover,\n",
    "                    producttype=\"S2MSI2A\",\n",
    "                )\n",
    "\n",
    "                matching_l2a_products_df = pd.DataFrame.from_dict(\n",
    "                    matching_l2a_products, orient=\"index\"\n",
    "                )\n",
    "                if len(matching_l2a_products_df) == 1:\n",
    "                    tile_log.info(matching_l2a_products_df.iloc[0, :][\"size\"])\n",
    "                    matching_l2a_products_df[\"size\"] = (\n",
    "                        matching_l2a_products_df[\"size\"]\n",
    "                        .str.split(\" \")\n",
    "                        .apply(\n",
    "                            lambda x: float(x[0])\n",
    "                            * {\"GB\": 1e3, \"MB\": 1, \"KB\": 1e-3}[x[1]]\n",
    "                        )\n",
    "                    )\n",
    "                    if (\n",
    "                        matching_l2a_products_df.iloc[0, :][\"size\"]\n",
    "                        > faulty_granule_threshold\n",
    "                    ):\n",
    "                        tile_log.info(\"Replacing L1C {} with L2A product:\".format(id))\n",
    "                        tile_log.info(\n",
    "                            \"              {}\".format(\n",
    "                                matching_l2a_products_df.iloc[0, :][\"title\"]\n",
    "                            )\n",
    "                        )\n",
    "                        drop.append(l1c_products.index[r])\n",
    "                        add.append(matching_l2a_products_df.iloc[0, :])\n",
    "                if len(matching_l2a_products_df) == 0:\n",
    "                    tile_log.info(\"Found no match for L1C: {}.\".format(id))\n",
    "                if len(matching_l2a_products_df) > 1:\n",
    "                    # check granule sizes on the server\n",
    "                    matching_l2a_products_df[\"size\"] = (\n",
    "                        matching_l2a_products_df[\"size\"]\n",
    "                        .str.split(\" \")\n",
    "                        .apply(\n",
    "                            lambda x: float(x[0])\n",
    "                            * {\"GB\": 1e3, \"MB\": 1, \"KB\": 1e-3}[x[1]]\n",
    "                        )\n",
    "                    )\n",
    "                    if (\n",
    "                        matching_l2a_products_df.iloc[0, :][\"size\"]\n",
    "                        > faulty_granule_threshold\n",
    "                    ):\n",
    "                        tile_log.info(\"Replacing L1C {} with L2A product:\".format(id))\n",
    "                        tile_log.info(\n",
    "                            \"              {}\".format(\n",
    "                                matching_l2a_products_df.iloc[0, :][\"title\"]\n",
    "                            )\n",
    "                        )\n",
    "                        drop.append(l1c_products.index[r])\n",
    "                        add.append(matching_l2a_products_df.iloc[0, :])\n",
    "\n",
    "            if len(drop) > 0:\n",
    "                l1c_products = l1c_products.drop(index=drop)\n",
    "            if len(add) > 0:\n",
    "                if config_dict[\"do_dev\"]:\n",
    "                    add = pd.DataFrame(add)\n",
    "                    l2a_products = pd.concat([l2a_products, add])\n",
    "                else:\n",
    "                    add = pd.DataFrame(add)\n",
    "                    l2a_products = pd.concat([l2a_products, add])\n",
    "\n",
    "            tile_log.info(\n",
    "                \"    {} L1C products remaining for download\".format(\n",
    "                    l1c_products.shape[0]\n",
    "                )\n",
    "            )\n",
    "    l2a_products = l2a_products.drop_duplicates(subset=\"title\")\n",
    "    tile_log.info(\"    {} L2A products remaining for download\".format(l2a_products.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24a4b6f",
   "metadata": {},
   "source": [
    "## <a id='toc4_4_'></a>[Download and Pre-Process L1C Change Imagery](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb442923",
   "metadata": {},
   "source": [
    "- If there any L1C products in the change images search query that are not matched with L2A products, then download these L1Cs and apply `atmospheric_correction`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "38ea3ce7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if config_dict[\"do_all\"] or config_dict[\"do_download\"]:\n",
    "    if l1c_products.shape[0] > 0:\n",
    "\n",
    "        tile_log.info(f\"Downloading Sentinel-2 L1C products from {download_source}\")\n",
    "\n",
    "        if download_source == \"scihub\":\n",
    "            queries_and_downloads.download_s2_data_from_df(\n",
    "                l1c_products,\n",
    "                l1_image_dir,\n",
    "                l2_image_dir,\n",
    "                download_source,\n",
    "                user=sen_user,\n",
    "                passwd=sen_pass,\n",
    "                try_scihub_on_fail=True,\n",
    "            )\n",
    "        elif download_source == \"dataspace\":\n",
    "                queries_and_downloads.download_s2_data_from_dataspace(\n",
    "                product_df=l1c_products,\n",
    "                l1c_directory=l1_image_dir,\n",
    "                l2a_directory=l2_image_dir,\n",
    "                dataspace_username=sen_user,\n",
    "                dataspace_password=sen_pass,\n",
    "                log=tile_log\n",
    "            )\n",
    "        else:\n",
    "            tile_log.error(f\"download source specified did not match 'scihub' or 'dataspace'\")\n",
    "            tile_log.error(f\"download source supplied was  :  {download_source}\")\n",
    "            tile_log.error(\"exiting pipeline...\")\n",
    "            sys.exit(1)\n",
    "\n",
    "    #     tile_log.info(\"Atmospheric correction with sen2cor.\")\n",
    "    #     raster_manipulation.atmospheric_correction(\n",
    "    #         l1_image_dir,\n",
    "    #         l2_image_dir,\n",
    "    #         sen2cor_path,\n",
    "    #         delete_unprocessed_image=False,\n",
    "    #         log=tile_log,\n",
    "    #     )\n",
    "\n",
    "    tile_log.info(\"Successfully downloaded the Sentinel-2 L1C products\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4490a3d",
   "metadata": {},
   "source": [
    "## <a id='toc4_5_'></a>[Download L2A Change Imagery](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7340dee5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if config_dict[\"do_all\"] or config_dict[\"do_download\"]:\n",
    "    if l2a_products.shape[0] > 0:\n",
    "        tile_log.info(f\"Downloading Sentinel-2 L2A products from {download_source}\")\n",
    "\n",
    "        if download_source == \"scihub\":\n",
    "            queries_and_downloads.download_s2_data(\n",
    "                l2a_products.to_dict(\"index\"),\n",
    "                l1_image_dir,\n",
    "                l2_image_dir,\n",
    "                download_source,\n",
    "                user=sen_user,\n",
    "                passwd=sen_pass,\n",
    "                try_scihub_on_fail=True,\n",
    "            )\n",
    "        if download_source == \"dataspace\":\n",
    "            queries_and_downloads.download_s2_data_from_dataspace(\n",
    "                product_df=l2a_products,\n",
    "                l1c_directory=l1_image_dir,\n",
    "                l2a_directory=l2_image_dir,\n",
    "                dataspace_username=sen_user,\n",
    "                dataspace_password=sen_pass,\n",
    "                log=tile_log\n",
    "            )\n",
    "\n",
    "    # check for incomplete L2A downloads and remove them\n",
    "    incomplete_downloads, sizes = raster_manipulation.find_small_safe_dirs(\n",
    "        l2_image_dir, threshold=faulty_granule_threshold * 1024 * 1024\n",
    "    )\n",
    "    if len(incomplete_downloads) > 0:\n",
    "        for index, safe_dir in enumerate(incomplete_downloads):\n",
    "            if sizes[\n",
    "                index\n",
    "            ] / 1024 / 1024 < faulty_granule_threshold and os.path.exists(safe_dir):\n",
    "                tile_log.warning(\n",
    "                    \"Found likely incomplete download of size {} MB: {}\".format(\n",
    "                        str(round(sizes[index] / 1024 / 1024)), safe_dir\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    tile_log.info(\"Successfully downloaded the Sentinel-2 L2A products\")\n",
    "\n",
    "    tile_log.info(\"---------------------------------------------------------------\")\n",
    "    tile_log.info(\"Image download and atmospheric correction for change detection images is complete.\")\n",
    "    tile_log.info(\"---------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce831c1e",
   "metadata": {},
   "source": [
    "### <a id='toc4_5_1_'></a>[Housekeeping - Compress L1Cs](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c9ba5c",
   "metadata": {},
   "source": [
    "If you have set your `do_zip` argument to `True`, then this cell will compress the L1Cs now that they have been atmospherically corrected and relabelled as L2As."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fd3db0ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if config_dict[\"do_all\"] or config_dict[\"do_download\"]:\n",
    "    if config_dict[\"do_delete\"]:\n",
    "        tile_log.info(\"---------------------------------------------------------------\")\n",
    "        tile_log.info(\"Deleting L1C images downloaded for change detection.\")\n",
    "        tile_log.info(\"Keeping only the derived L2A images after atmospheric correction.\")\n",
    "        tile_log.info(\"---------------------------------------------------------------\")\n",
    "        directory = l1_image_dir\n",
    "        tile_log.info(\"Deleting {}\".format(directory))\n",
    "        shutil.rmtree(directory)\n",
    "        tile_log.info(\"---------------------------------------------------------------\")\n",
    "        tile_log.info(\"Deletion complete\")\n",
    "        tile_log.info(\"---------------------------------------------------------------\")\n",
    "    else:\n",
    "        if config_dict[\"do_zip\"]:\n",
    "            tile_log.info(\"---------------------------------------------------------------\")\n",
    "            tile_log.info(\"Zipping L1C images downloaded for change detection\")\n",
    "            tile_log.info(\"---------------------------------------------------------------\")\n",
    "            filesystem_utilities.zip_contents(l1_image_dir)\n",
    "            tile_log.info(\"---------------------------------------------------------------\")\n",
    "            tile_log.info(\"Zipping complete\")\n",
    "            tile_log.info(\"---------------------------------------------------------------\")\n",
    "\n",
    "    tile_log.info(\"Cell successfully finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9d8827",
   "metadata": {},
   "source": [
    "## <a id='toc4_6_'></a>[Cloud Masking, Offsetting and Quicklooks](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dfa11e",
   "metadata": {},
   "source": [
    "Here, like before in the previous session, we cloud mask, apply the baseline offset correction and produce quicklooks (if selected).  \n",
    "\n",
    "Additionally, if you have set the `do_zip` flag to True, then `pyeo` will compress the cloud masked L2A images, as we no longer need these once classified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ca3e0d-2fea-4d33-a21b-109d42b03c67",
   "metadata": {},
   "source": [
    "### Cloud Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3000a7f4-43b0-414e-8991-e596ede13c09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if config_dict[\"do_all\"] or config_dict[\"do_download\"]:\n",
    "    tile_log.info(\"---------------------------------------------------------------\")\n",
    "    tile_log.info(\n",
    "        \"Applying simple cloud, cloud shadow and haze mask based on SCL files and stacking the masked band raster files.\"\n",
    "    )\n",
    "    tile_log.info(\"---------------------------------------------------------------\")\n",
    "\n",
    "    raster_manipulation.apply_scl_cloud_mask(\n",
    "        l2_image_dir,\n",
    "        l2_masked_image_dir,\n",
    "        scl_classes=[0, 1, 2, 3, 8, 9, 10, 11],\n",
    "        buffer_size=buffer_size,\n",
    "        bands=bands,\n",
    "        out_resolution=out_resolution,\n",
    "        haze=None,\n",
    "        epsg=epsg,\n",
    "        skip_existing=skip_existing,\n",
    "    )\n",
    "\n",
    "    tile_log.info(\"---------------------------------------------------------------\")\n",
    "    tile_log.info(\"Cloud masking and band stacking of new L2A images are complete.\")\n",
    "    tile_log.info(\"---------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adac6cee-3923-4eeb-b6a8-f5565bcf2521",
   "metadata": {},
   "source": [
    "### Offsetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5e75c142-0712-46c9-ac9a-c438c3bd21e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if config_dict[\"do_all\"] or config_dict[\"do_download\"]:\n",
    "    tile_log.info(\"---------------------------------------------------------------\")\n",
    "    tile_log.info(\"Offsetting cloud masked L2A images.\")\n",
    "    tile_log.info(\"---------------------------------------------------------------\")\n",
    "\n",
    "    raster_manipulation.apply_processing_baseline_offset_correction_to_tiff_file_directory(\n",
    "        l2_masked_image_dir, l2_masked_image_dir)\n",
    "\n",
    "    tile_log.info(\"---------------------------------------------------------------\")\n",
    "    tile_log.info(\"Offsetting of cloud masked L2A images complete.\")\n",
    "    tile_log.info(\"---------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93749bf8-f962-4e75-b519-d772bcb5428c",
   "metadata": {},
   "source": [
    "###Â Quicklooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "37aa6e6c-ab81-4f7f-ad6f-d4f8dc325cf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if config_dict[\"do_all\"] or config_dict[\"do_download\"]:\n",
    "    if config_dict[\"do_quicklooks\"] or config_dict[\"do_all\"]:\n",
    "        tile_log.info(\n",
    "            \"---------------------------------------------------------------\"\n",
    "        )\n",
    "        tile_log.info(\"Producing quicklooks.\")\n",
    "        tile_log.info(\n",
    "            \"---------------------------------------------------------------\"\n",
    "        )\n",
    "        dirs_for_quicklooks = [l2_masked_image_dir]\n",
    "        for main_dir in dirs_for_quicklooks:\n",
    "            files = [\n",
    "                f.path\n",
    "                for f in os.scandir(main_dir)\n",
    "                if f.is_file() and os.path.basename(f).endswith(\".tif\")\n",
    "            ]\n",
    "            if len(files) == 0:\n",
    "                tile_log.warning(\"No images found in {}.\".format(main_dir))\n",
    "            else:\n",
    "                for f in files:\n",
    "                    quicklook_path = os.path.join(\n",
    "                        quicklook_dir,\n",
    "                        os.path.basename(f).split(\".\")[0] + \".png\",\n",
    "                    )\n",
    "                    tile_log.info(\"Creating quicklook: {}\".format(quicklook_path))\n",
    "                    raster_manipulation.create_quicklook(\n",
    "                        f,\n",
    "                        quicklook_path,\n",
    "                        width=512,\n",
    "                        height=512,\n",
    "                        format=\"PNG\",\n",
    "                        bands=[3, 2, 1],\n",
    "                        scale_factors=[[0, 2000, 0, 255]],\n",
    "                    )\n",
    "        tile_log.info(\"Quicklooks complete.\")\n",
    "    else:\n",
    "        tile_log.info(\"Quicklooks disabled in ini file.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a767a42f-5983-41bc-a0eb-97b971d4a11e",
   "metadata": {},
   "source": [
    "### Housekeeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8314ea16-18f7-4bb8-a720-ba14c69535b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if config_dict[\"do_all\"] or config_dict[\"do_download\"]:\n",
    "    if config_dict[\"do_zip\"]:\n",
    "        tile_log.info(\n",
    "            \"---------------------------------------------------------------\"\n",
    "        )\n",
    "        tile_log.info(\"Zipping L2A images downloaded for change detection\")\n",
    "        tile_log.info(\n",
    "            \"---------------------------------------------------------------\"\n",
    "        )\n",
    "        filesystem_utilities.zip_contents(l2_image_dir)\n",
    "        tile_log.info(\n",
    "            \"---------------------------------------------------------------\"\n",
    "        )\n",
    "        tile_log.info(\"Zipping complete\")\n",
    "        tile_log.info(\n",
    "            \"---------------------------------------------------------------\"\n",
    "        )\n",
    "\n",
    "    tile_log.info(\"---------------------------------------------------------------\")\n",
    "    tile_log.info(\n",
    "        \"Compressing tiff files in directory {} and all subdirectories\".format(\n",
    "            l2_masked_image_dir\n",
    "        )\n",
    "    )\n",
    "    tile_log.info(\"---------------------------------------------------------------\")\n",
    "    for root, dirs, files in os.walk(l2_masked_image_dir):\n",
    "        all_tiffs = [\n",
    "            image_name for image_name in files if image_name.endswith(\".tif\")\n",
    "        ]\n",
    "        for this_tiff in all_tiffs:\n",
    "            raster_manipulation.compress_tiff(\n",
    "                os.path.join(root, this_tiff), os.path.join(root, this_tiff)\n",
    "            )\n",
    "\n",
    "    tile_log.info(\"---------------------------------------------------------------\")\n",
    "    tile_log.info(\n",
    "        \"Pre-processing of change detection images, file compression, zipping\"\n",
    "    )\n",
    "    tile_log.info(\n",
    "        \"and deletion of intermediate file products (if selected) are complete.\"\n",
    "    )\n",
    "    tile_log.info(\"---------------------------------------------------------------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0abe8bf",
   "metadata": {},
   "source": [
    "## <a id='toc4_7_'></a>[Classification of the Baseline Composite and Change Images](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42244da4",
   "metadata": {},
   "source": [
    "Here, we classify the Baseline Composite and the Change images using the model we created in the model training session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8c13b672",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "    if config_dict[\"do_all\"] or config_dict[\"do_classify\"]:\n",
    "        tile_log.info(\"---------------------------------------------------------------\")\n",
    "        tile_log.info(\n",
    "            \"Classify a land cover map for each L2A image and composite image using a saved model\"\n",
    "        )\n",
    "        tile_log.info(\"---------------------------------------------------------------\")\n",
    "        tile_log.info(\"Model used: {}\".format(model_path))\n",
    "        if skip_existing:\n",
    "            tile_log.info(\"Skipping existing classification images if found.\")\n",
    "        classification.classify_directory(\n",
    "            composite_dir,\n",
    "            model_path,\n",
    "            categorised_image_dir,\n",
    "            prob_out_dir=None,\n",
    "            apply_mask=False,\n",
    "            out_type=\"GTiff\",\n",
    "            chunks=config_dict[\"chunks\"],\n",
    "            skip_existing=skip_existing,\n",
    "        )\n",
    "        classification.classify_directory(\n",
    "            l2_masked_image_dir,\n",
    "            model_path,\n",
    "            categorised_image_dir,\n",
    "            prob_out_dir=None,\n",
    "            apply_mask=False,\n",
    "            out_type=\"GTiff\",\n",
    "            chunks=config_dict[\"chunks\"],\n",
    "            skip_existing=skip_existing,\n",
    "        )\n",
    "\n",
    "        tile_log.info(\"---------------------------------------------------------------\")\n",
    "        tile_log.info(\n",
    "            \"Compressing tiff files in directory {} and all subdirectories\".format(\n",
    "                categorised_image_dir\n",
    "            )\n",
    "        )\n",
    "        tile_log.info(\"---------------------------------------------------------------\")\n",
    "        for root, dirs, files in os.walk(categorised_image_dir):\n",
    "            all_tiffs = [\n",
    "                image_name for image_name in files if image_name.endswith(\".tif\")\n",
    "            ]\n",
    "            for this_tiff in all_tiffs:\n",
    "                raster_manipulation.compress_tiff(\n",
    "                    os.path.join(root, this_tiff), os.path.join(root, this_tiff)\n",
    "                )\n",
    "\n",
    "        tile_log.info(\"---------------------------------------------------------------\")\n",
    "        tile_log.info(\"Classification of all images is complete.\")\n",
    "        tile_log.info(\"---------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2711cffd-1bb2-4fe2-96d8-2d50b777ae88",
   "metadata": {},
   "source": [
    "### Housekeeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ef962a98-5d2e-48e2-825d-2af21c94e2b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if config_dict[\"do_all\"] or config_dict[\"do_classify\"]:\n",
    "    if config_dict[\"do_quicklooks\"] or config_dict[\"do_all\"]:\n",
    "        tile_log.info(\n",
    "            \"---------------------------------------------------------------\"\n",
    "        )\n",
    "        tile_log.info(\"Producing quicklooks.\")\n",
    "        tile_log.info(\n",
    "            \"---------------------------------------------------------------\"\n",
    "        )\n",
    "        dirs_for_quicklooks = [categorised_image_dir]\n",
    "        for main_dir in dirs_for_quicklooks:\n",
    "            # files = [ f.path for f in os.scandir(main_dir) if f.is_file() and os.path.basename(f).endswith(\".tif\") ]\n",
    "            files = [\n",
    "                f.path\n",
    "                for f in os.scandir(main_dir)\n",
    "                if f.is_file()\n",
    "                and os.path.basename(f).endswith(\".tif\")\n",
    "                and \"class\" in os.path.basename(f)\n",
    "            ]  # do classification images only\n",
    "            if len(files) == 0:\n",
    "                tile_log.warning(\"No images found in {}.\".format(main_dir))\n",
    "            else:\n",
    "                for f in files:\n",
    "                    quicklook_path = os.path.join(\n",
    "                        quicklook_dir,\n",
    "                        os.path.basename(f).split(\".\")[0] + \".png\",\n",
    "                    )\n",
    "                    tile_log.info(\"Creating quicklook: {}\".format(quicklook_path))\n",
    "                    raster_manipulation.create_quicklook(\n",
    "                        f, quicklook_path, width=512, height=512, format=\"PNG\"\n",
    "                    )\n",
    "        tile_log.info(\"Quicklooks complete.\")\n",
    "    else:\n",
    "        tile_log.info(\"Quicklooks disabled in ini file.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6923b743",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <a id='toc4_8_'></a>[Change Detection](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab11aae",
   "metadata": {},
   "source": [
    "To perform Change Detection, we take the Classified Change Imagery and compare it with the Classified Baseline Composite.\n",
    "\n",
    "Because we are concerned with monitoring deforestation for our Change Detection, `pyeo` examines whether any forest classes (*classes 1, 11 and 12*) change to non-forest classes (*classes 3, 4, 5 and 13*).  \n",
    "\n",
    "As new change imagery becomes available (*as deforestation monitoring is an iterative process through time*), these change images are classified and compared to the baseline, again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404f0d36",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a08c081",
   "metadata": {},
   "source": [
    "The overall Change Detection can be summarised as this:\n",
    "- PyEO first looks for the composite and change imagery classifications and orders them by most recent.\n",
    "- Then, it searches for existing report files created from previous PyEO runs and archive them, moving them to an archived folder.\n",
    "- Then it creates the change report by sequentially comparing the classified change imagery against the classified baseline composite.\n",
    "- Once finished, PyEO does some housekeeping, compressing unneeded files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "484d5345-54bd-4d75-84ef-7f9a27274e5c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-29 08:21:23,996: INFO: ---------------------------------------------------------------\n",
      "2023-06-29 08:21:23,997: INFO:                     ****PROCESSING START****\n",
      "2023-06-29 08:21:23,998: INFO: ---------------------------------------------------------------\n",
      "2023-06-29 08:21:23,999: INFO: ----------------------------------------\n",
      "2023-06-29 08:21:23,999: INFO: Starting Vectorisation of the Change Report Raster of Tile: 36NXG\n",
      "2023-06-29 08:21:24,000: INFO: ----------------------------------------\n",
      "2023-06-29 08:21:24,002: INFO: what is change_report_path  :  /home/sepal-user/20230626_pyeo_installation/36NXG/output/probabilities/report_20221202T075301_36NXG_20230317T074649.tif\n",
      "2023-06-29 08:21:24,123: INFO: Opening /home/sepal-user/20230626_pyeo_installation/36NXG/output/probabilities/report_20221202T075301_36NXG_20230317T074649.tif\n",
      "2023-06-29 08:21:24,124: INFO: Successfully opened /home/sepal-user/20230626_pyeo_installation/36NXG/output/probabilities/report_20221202T075301_36NXG_20230317T074649.tif\n",
      "2023-06-29 08:21:24,211: INFO: Now vectorising the raster band\n",
      "2023-06-29 08:21:58,693: INFO: Band 15 of /home/sepal-user/20230626_pyeo_installation/36NXG/output/probabilities/report_20221202T075301_36NXG_20230317T074649.tif was successfully vectorised\n",
      "2023-06-29 08:21:58,698: INFO: Band 15 was written to /home/sepal-user/20230626_pyeo_installation/36NXG/output/probabilities/report_20221202T075301_36NXG_20230317T074649_band15.shp\n",
      "2023-06-29 08:21:58,755: INFO: filtering out zeroes and nodata from: /home/sepal-user/20230626_pyeo_installation/36NXG/output/probabilities/report_20221202T075301_36NXG_20230317T074649_band15.shp\n",
      "2023-06-29 08:22:05,811: INFO: filtering complete and saved at  : /home/sepal-user/20230626_pyeo_installation/36NXG/output/probabilities/report_20221202T075301_36NXG_20230317T074649_band15_filtered.shp\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "merge_and_calculate_spatial() got an unexpected keyword argument 'write_kmlfile'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 333\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_all\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_vectorise\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyeo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapps\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01macd_national\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01macd_by_tile_vectorisation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m vector_report_generation\n\u001b[0;32m--> 333\u001b[0m     output_vector_products \u001b[38;5;241m=\u001b[39m \u001b[43mvector_report_generation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtile_to_process\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    335\u001b[0m     tile_log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m---------------------------------------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    336\u001b[0m     tile_log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReport image vectorised. Output file(s) created:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/20230626_pyeo_installation/pyeo/pyeo/apps/acd_national/acd_by_tile_vectorisation.py:113\u001b[0m, in \u001b[0;36mvector_report_generation\u001b[0;34m(config_path, tile)\u001b[0m\n\u001b[1;32m    103\u001b[0m rb_first_changedate_zstats_df \u001b[38;5;241m=\u001b[39m vectorisation\u001b[38;5;241m.\u001b[39mzonal_statistics(\n\u001b[1;32m    104\u001b[0m     raster_path\u001b[38;5;241m=\u001b[39mchange_report_path,\n\u001b[1;32m    105\u001b[0m     shapefile_path\u001b[38;5;241m=\u001b[39mpath_vectorised_binary_filtered,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    108\u001b[0m     config_dict\u001b[38;5;241m=\u001b[39mconfig_dict,\n\u001b[1;32m    109\u001b[0m )\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# was band=7\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# table joins, area, lat lon, county\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m output_vector_files \u001b[38;5;241m=\u001b[39m \u001b[43mvectorisation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge_and_calculate_spatial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mrb_ndetections_zstats_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrb_ndetections_zstats_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mrb_confidence_zstats_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrb_confidence_zstats_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mrb_first_changedate_zstats_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrb_first_changedate_zstats_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mpath_to_vectorised_binary_filtered\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_vectorised_binary_filtered\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mwrite_csv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mwrite_shapefile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mwrite_kmlfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mwrite_pkl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mchange_report_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchange_report_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mlog\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtile_log\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mepsg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mlevel_1_boundaries_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel_1_boundaries_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mtileid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m                                    \u001b[49m\u001b[38;5;66;43;03m# conda_env_name=conda_env_name,\u001b[39;49;00m\n\u001b[1;32m    128\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mdelete_intermediates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m                                \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m tile_log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m---------------------------------------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    133\u001b[0m tile_log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVectorisation of the Change Report Raster complete\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: merge_and_calculate_spatial() got an unexpected keyword argument 'write_kmlfile'"
     ]
    }
   ],
   "source": [
    "if config_dict[\"do_all\"] or config_dict[\"do_change\"]:\n",
    "    tile_log.info(\"---------------------------------------------------------------\")\n",
    "    tile_log.info(\"Creating change layers from stacked class images.\")\n",
    "    tile_log.info(\"---------------------------------------------------------------\")\n",
    "    tile_log.info(\"Changes of interest:\")\n",
    "    tile_log.info(\n",
    "        \"  from any of the classes {}\".format(config_dict[\"from_classes\"])\n",
    "    )\n",
    "    tile_log.info(\"  to   any of the classes {}\".format(config_dict[\"to_classes\"]))\n",
    "\n",
    "    # optionally sieve the class images\n",
    "    if sieve > 0:\n",
    "        tile_log.info(\"Applying sieve to classification outputs.\")\n",
    "        sieved_paths = raster_manipulation.sieve_directory(\n",
    "            in_dir=categorised_image_dir,\n",
    "            out_dir=sieved_image_dir,\n",
    "            neighbours=8,\n",
    "            sieve=sieve,\n",
    "            out_type=\"GTiff\",\n",
    "            skip_existing=skip_existing,\n",
    "        )\n",
    "        # if sieve was chosen, work with the sieved class images\n",
    "        class_image_dir = sieved_image_dir\n",
    "    else:\n",
    "        # if sieve was not chosen, work with the original class images\n",
    "        class_image_dir = categorised_image_dir\n",
    "\n",
    "    # get all image paths in the classification maps directory except the class composites\n",
    "    class_image_paths = [\n",
    "        f.path\n",
    "        for f in os.scandir(class_image_dir)\n",
    "        if f.is_file() and f.name.endswith(\".tif\") and not \"composite_\" in f.name\n",
    "    ]\n",
    "    if len(class_image_paths) == 0:\n",
    "        raise FileNotFoundError(\n",
    "            \"No class images found in {}.\".format(class_image_dir)\n",
    "        )\n",
    "\n",
    "    # sort class images by image acquisition date\n",
    "    class_image_paths = list(\n",
    "        filter(filesystem_utilities.get_image_acquisition_time, class_image_paths)\n",
    "    )\n",
    "    class_image_paths.sort(\n",
    "        key=lambda x: filesystem_utilities.get_image_acquisition_time(x)\n",
    "    )\n",
    "    for index, image in enumerate(class_image_paths):\n",
    "        tile_log.info(\"{}: {}\".format(index, image))\n",
    "\n",
    "    # find the latest available composite\n",
    "    try:\n",
    "        latest_composite_name = filesystem_utilities.sort_by_timestamp(\n",
    "            [\n",
    "                image_name\n",
    "                for image_name in os.listdir(composite_dir)\n",
    "                if image_name.endswith(\".tif\")\n",
    "            ],\n",
    "            recent_first=True,\n",
    "        )[0]\n",
    "        latest_composite_path = os.path.join(composite_dir, latest_composite_name)\n",
    "        tile_log.info(\"Most recent composite at {}\".format(latest_composite_path))\n",
    "    except IndexError:\n",
    "        tile_log.critical(\n",
    "            \"Latest composite not found. The first time you run this script, you need to include the \"\n",
    "            \"--build-composite flag to create a base composite to work off. If you have already done this,\"\n",
    "            \"check that the earliest dated image in your images/merged folder is later than the earliest\"\n",
    "            \" dated image in your composite/ folder.\"\n",
    "        )\n",
    "        sys.exit(1)\n",
    "\n",
    "    latest_class_composite_path = os.path.join(\n",
    "        class_image_dir,\n",
    "        [\n",
    "            f.path\n",
    "            for f in os.scandir(class_image_dir)\n",
    "            if f.is_file()\n",
    "            and os.path.basename(latest_composite_path)[:-4] in f.name\n",
    "            and f.name.endswith(\".tif\")\n",
    "        ][0],\n",
    "    )\n",
    "\n",
    "    tile_log.info(\n",
    "        \"Most recent class composite at {}\".format(latest_class_composite_path)\n",
    "    )\n",
    "    if not os.path.exists(latest_class_composite_path):\n",
    "        tile_log.critical(\n",
    "            \"Latest class composite not found. The first time you run this script, you need to include the \"\n",
    "            \"--build-composite flag to create a base composite to work off. If you have already done this,\"\n",
    "            \"check that the earliest dated image in your images/merged folder is later than the earliest\"\n",
    "            \" dated image in your composite/ folder. Then, you need to run the --classify option.\"\n",
    "        )\n",
    "        sys.exit(1)\n",
    "\n",
    "    if config_dict[\n",
    "        \"do_dev\"\n",
    "    ]:  # set the name of the report file in the development version run\n",
    "        before_timestamp = filesystem_utilities.get_change_detection_dates(\n",
    "            os.path.basename(latest_class_composite_path)\n",
    "        )[0]\n",
    "        # I.R. 20220611 START\n",
    "        ## Timestamp report with the date of most recent classified image that contributes to it\n",
    "        after_timestamp = filesystem_utilities.get_image_acquisition_time(\n",
    "            os.path.basename(class_image_paths[-1])\n",
    "        )\n",
    "        ## ORIGINAL\n",
    "        # gets timestamp of the earliest change image of those available in class_image_path\n",
    "        # after_timestamp  = pyeo.filesystem_utilities.get_image_acquisition_time(os.path.basename(class_image_paths[0]))\n",
    "        # I.R. 20220611 END\n",
    "        output_product = os.path.join(\n",
    "            probability_image_dir,\n",
    "            \"report_{}_{}_{}.tif\".format(\n",
    "                before_timestamp.strftime(\"%Y%m%dT%H%M%S\"),\n",
    "                tile_to_process,\n",
    "                after_timestamp.strftime(\"%Y%m%dT%H%M%S\"),\n",
    "            ),\n",
    "        )\n",
    "        tile_log.info(\"I.R. Report file name will be {}\".format(output_product))\n",
    "\n",
    "        # if a report file exists, archive it  ( I.R. Changed from 'rename it to show it has been updated')\n",
    "        n_report_files = len(\n",
    "            [\n",
    "                f\n",
    "                for f in os.scandir(probability_image_dir)\n",
    "                if f.is_file()\n",
    "                and f.name.startswith(\"report_\")\n",
    "                and f.name.endswith(\".tif\")\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        if n_report_files > 0:\n",
    "            # I.R. ToDo: Should iterate over output_product_existing in case more than one report file is present (though unlikely)\n",
    "            output_product_existing = [\n",
    "                f.path\n",
    "                for f in os.scandir(probability_image_dir)\n",
    "                if f.is_file()\n",
    "                and f.name.startswith(\"report_\")\n",
    "                and f.name.endswith(\".tif\")\n",
    "            ][0]\n",
    "            tile_log.info(\n",
    "                \"Found existing report image product: {}\".format(\n",
    "                    output_product_existing\n",
    "                )\n",
    "            )\n",
    "\n",
    "            output_product_existing_archived = os.path.join(\n",
    "                os.path.dirname(output_product_existing),\n",
    "                \"archived_\" + os.path.basename(output_product_existing),\n",
    "            )\n",
    "            tile_log.info(\n",
    "                \"Renaming existing report image product to: {}\".format(\n",
    "                    output_product_existing_archived\n",
    "                )\n",
    "            )\n",
    "            os.rename(output_product_existing, output_product_existing_archived)\n",
    "\n",
    "    # find change patterns in the stack of classification images\n",
    "    for index, image in enumerate(class_image_paths):\n",
    "        tile_log.info(\"\")\n",
    "        tile_log.info(\"\")\n",
    "        tile_log.info(f\"  printing index, image   : {index}, {image}\")\n",
    "        tile_log.info(\"\")\n",
    "        tile_log.info(\"\")\n",
    "        before_timestamp = filesystem_utilities.get_change_detection_dates(\n",
    "            os.path.basename(latest_class_composite_path)\n",
    "        )[0]\n",
    "        after_timestamp = filesystem_utilities.get_image_acquisition_time(\n",
    "            os.path.basename(image)\n",
    "        )\n",
    "        tile_log.info(\n",
    "            \"*** PROCESSING CLASSIFIED IMAGE: {} of {} filename: {} ***\".format(\n",
    "                index, len(class_image_paths), image\n",
    "            )\n",
    "        )\n",
    "        tile_log.info(\"  early time stamp: {}\".format(before_timestamp))\n",
    "        tile_log.info(\"  late  time stamp: {}\".format(after_timestamp))\n",
    "        change_raster = os.path.join(\n",
    "            probability_image_dir,\n",
    "            \"change_{}_{}_{}.tif\".format(\n",
    "                before_timestamp.strftime(\"%Y%m%dT%H%M%S\"),\n",
    "                tile_to_process,\n",
    "                after_timestamp.strftime(\"%Y%m%dT%H%M%S\"),\n",
    "            ),\n",
    "        )\n",
    "        tile_log.info(\n",
    "            \"  Change raster file to be created: {}\".format(change_raster)\n",
    "        )\n",
    "\n",
    "        dNDVI_raster = os.path.join(\n",
    "            probability_image_dir,\n",
    "            \"dNDVI_{}_{}_{}.tif\".format(\n",
    "                before_timestamp.strftime(\"%Y%m%dT%H%M%S\"),\n",
    "                tile_to_process,\n",
    "                after_timestamp.strftime(\"%Y%m%dT%H%M%S\"),\n",
    "            ),\n",
    "        )\n",
    "        tile_log.info(\n",
    "            \"  I.R. dNDVI raster file to be created: {}\".format(dNDVI_raster)\n",
    "        )\n",
    "\n",
    "        NDVI_raster = os.path.join(\n",
    "            probability_image_dir,\n",
    "            \"NDVI_{}_{}_{}.tif\".format(\n",
    "                before_timestamp.strftime(\"%Y%m%dT%H%M%S\"),\n",
    "                tile_to_process,\n",
    "                after_timestamp.strftime(\"%Y%m%dT%H%M%S\"),\n",
    "            ),\n",
    "        )\n",
    "        tile_log.info(\n",
    "            \"  I.R. NDVI raster file of change image to be created: {}\".format(\n",
    "                NDVI_raster\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if config_dict[\"do_dev\"]:\n",
    "            # This function looks for changes from class 'change_from' in the composite to any of the 'change_to_classes'\n",
    "            # in the change images. Pixel values are the acquisition date of the detected change of interest or zero.\n",
    "            # TODO: In change_from_class_maps(), add a flag (e.g. -1) whether a pixel was a cloud in the later image.\n",
    "            # Applying check whether dNDVI < -0.2, i.e. greenness has decreased over changed areas\n",
    "\n",
    "            tile_log.info(\"Update of the report image product based on change detection image.\")\n",
    "            raster_manipulation.__change_from_class_maps(\n",
    "                old_class_path=latest_class_composite_path,\n",
    "                new_class_path=image,\n",
    "                change_raster=change_raster,\n",
    "                dNDVI_raster=dNDVI_raster,\n",
    "                NDVI_raster=NDVI_raster,\n",
    "                change_from=from_classes,\n",
    "                change_to=to_classes,\n",
    "                report_path=output_product,\n",
    "                skip_existing=skip_existing,\n",
    "                old_image_dir=composite_dir,\n",
    "                new_image_dir=l2_masked_image_dir,\n",
    "                viband1=4,\n",
    "                viband2=3,\n",
    "                dNDVI_threshold=-0.2,\n",
    "                log=tile_log,\n",
    "            )\n",
    "        else:\n",
    "            raster_manipulation.change_from_class_maps(\n",
    "                latest_class_composite_path,\n",
    "                image,\n",
    "                change_raster,\n",
    "                change_from=from_classes,\n",
    "                change_to=to_classes,\n",
    "                skip_existing=skip_existing,\n",
    "            )\n",
    "\n",
    "    tile_log.info(\"---------------------------------------------------------------\")\n",
    "    tile_log.info(\"Post-classification change detection complete.\")\n",
    "    tile_log.info(\"---------------------------------------------------------------\")\n",
    "\n",
    "    tile_log.info(\"---------------------------------------------------------------\")\n",
    "    tile_log.info(\n",
    "        \"Compressing tiff files in directory {} and all subdirectories\".format(\n",
    "            probability_image_dir\n",
    "        )\n",
    "    )\n",
    "    tile_log.info(\"---------------------------------------------------------------\")\n",
    "    for root, dirs, files in os.walk(probability_image_dir):\n",
    "        all_tiffs = [\n",
    "            image_name for image_name in files if image_name.endswith(\".tif\")\n",
    "        ]\n",
    "        for this_tiff in all_tiffs:\n",
    "            raster_manipulation.compress_tiff(\n",
    "                os.path.join(root, this_tiff), os.path.join(root, this_tiff)\n",
    "            )\n",
    "\n",
    "    tile_log.info(\"---------------------------------------------------------------\")\n",
    "    tile_log.info(\n",
    "        \"Compressing tiff files in directory {} and all subdirectories\".format(\n",
    "            sieved_image_dir\n",
    "        )\n",
    "    )\n",
    "    tile_log.info(\"---------------------------------------------------------------\")\n",
    "    for root, dirs, files in os.walk(sieved_image_dir):\n",
    "        all_tiffs = [\n",
    "            image_name for image_name in files if image_name.endswith(\".tif\")\n",
    "        ]\n",
    "        for this_tiff in all_tiffs:\n",
    "            raster_manipulation.compress_tiff(\n",
    "                os.path.join(root, this_tiff), os.path.join(root, this_tiff)\n",
    "            )\n",
    "\n",
    "    if not config_dict[\"do_dev\"]:\n",
    "        tile_log.info(\n",
    "            \"---------------------------------------------------------------\"\n",
    "        )\n",
    "        tile_log.info(\n",
    "            \"Creating aggregated report file. Deprecated in the development version.\"\n",
    "        )\n",
    "        tile_log.info(\n",
    "            \"---------------------------------------------------------------\"\n",
    "        )\n",
    "        # combine all change layers into one output raster with two layers:\n",
    "        #   (1) pixels show the earliest change detection date (expressed as the number of days since 1/1/2000)\n",
    "        #   (2) pixels show the number of change detection dates (summed up over all change images in the folder)\n",
    "        date_image_paths = [\n",
    "            f.path\n",
    "            for f in os.scandir(probability_image_dir)\n",
    "            if f.is_file() and f.name.endswith(\".tif\") and \"change_\" in f.name\n",
    "        ]\n",
    "        if len(date_image_paths) == 0:\n",
    "            raise FileNotFoundError(\n",
    "                \"No class images found in {}.\".format(categorised_image_dir)\n",
    "            )\n",
    "\n",
    "        before_timestamp = filesystem_utilities.get_change_detection_dates(\n",
    "            os.path.basename(latest_class_composite_path)\n",
    "        )[0]\n",
    "        after_timestamp = filesystem_utilities.get_image_acquisition_time(\n",
    "            os.path.basename(class_image_paths[-1])\n",
    "        )\n",
    "        output_product = os.path.join(\n",
    "            probability_image_dir,\n",
    "            \"report_{}_{}_{}.tif\".format(\n",
    "                before_timestamp.strftime(\"%Y%m%dT%H%M%S\"),\n",
    "                tile_to_process,\n",
    "                # tile_id,\n",
    "                after_timestamp.strftime(\"%Y%m%dT%H%M%S\"),\n",
    "            ),\n",
    "        )\n",
    "        tile_log.info(\"Combining date maps: {}\".format(date_image_paths))\n",
    "        raster_manipulation.combine_date_maps(date_image_paths, output_product)\n",
    "\n",
    "    tile_log.info(\"---------------------------------------------------------------\")\n",
    "    tile_log.info(\n",
    "        \"Report image product completed / updated: {}\".format(output_product)\n",
    "    )\n",
    "        \n",
    "if config_dict[\"do_all\"] or config_dict[\"do_vectorise\"]:\n",
    "    from pyeo.apps.acd_national.acd_by_tile_vectorisation import vector_report_generation\n",
    "    output_vector_products = vector_report_generation(config_path, tile_to_process)\n",
    "\n",
    "    tile_log.info(\"---------------------------------------------------------------\")\n",
    "    tile_log.info(\"Report image vectorised. Output file(s) created:\")\n",
    "    for i in range(len(output_vector_products)):\n",
    "        tile_log.info(\"  {}\".format(output_vector_products[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b628e59",
   "metadata": {},
   "source": [
    "### <a id='toc4_8_2_'></a>[Final Housekeeping](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa8e924",
   "metadata": {},
   "source": [
    "Finally, we run some more housekeeping, deleting or compressing unnecessary files, depending on the argument supplied at the beginning of this session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67d0921",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if config_dict[\"do_all\"] or config_dict[\"do_change\"]:\n",
    "    tile_log.info(\"Compressing the report image.\")\n",
    "    tile_log.info(\"---------------------------------------------------------------\")\n",
    "    raster_manipulation.compress_tiff(output_product, output_product)\n",
    "\n",
    "    if config_dict[\"do_delete\"]:\n",
    "        tile_log.info(\n",
    "            \"---------------------------------------------------------------\"\n",
    "        )\n",
    "        tile_log.info(\n",
    "            \"Deleting intermediate class images used in change detection.\"\n",
    "        )\n",
    "        tile_log.info(\n",
    "            \"They can be recreated from the cloud-masked, band-stacked L2A images and the saved model.\"\n",
    "        )\n",
    "        tile_log.info(\n",
    "            \"---------------------------------------------------------------\"\n",
    "        )\n",
    "        directories = [\n",
    "            categorised_image_dir,\n",
    "            sieved_image_dir,\n",
    "            probability_image_dir,\n",
    "        ]\n",
    "        for directory in directories:\n",
    "            paths = [f for f in os.listdir(directory)]\n",
    "            for f in paths:\n",
    "                # keep the classified composite layers and the report image product for the next change detection\n",
    "                if not f.startswith(\"composite_\") and not f.startswith(\"report_\"):\n",
    "                    tile_log.info(\"Deleting {}\".format(os.path.join(directory, f)))\n",
    "                    if os.path.isdir(os.path.join(directory, f)):\n",
    "                        shutil.rmtree(os.path.join(directory, f))\n",
    "                    else:\n",
    "                        os.remove(os.path.join(directory, f))\n",
    "        tile_log.info(\n",
    "            \"---------------------------------------------------------------\"\n",
    "        )\n",
    "        tile_log.info(\"Deletion of intermediate file products complete.\")\n",
    "        tile_log.info(\n",
    "            \"---------------------------------------------------------------\"\n",
    "        )\n",
    "    else:\n",
    "        if config_dict[\"do_zip\"]:\n",
    "            tile_log.info(\n",
    "                \"---------------------------------------------------------------\"\n",
    "            )\n",
    "            tile_log.info(\n",
    "                \"Zipping intermediate class images used in change detection\"\n",
    "            )\n",
    "            tile_log.info(\n",
    "                \"---------------------------------------------------------------\"\n",
    "            )\n",
    "            directories = [categorised_image_dir, sieved_image_dir]\n",
    "            for directory in directories:\n",
    "                filesystem_utilities.zip_contents(\n",
    "                    directory, notstartswith=[\"composite_\", \"report_\"]\n",
    "                )\n",
    "            tile_log.info(\n",
    "                \"---------------------------------------------------------------\"\n",
    "            )\n",
    "            tile_log.info(\"Zipping complete\")\n",
    "            tile_log.info(\n",
    "                \"---------------------------------------------------------------\"\n",
    "            )\n",
    "\n",
    "    tile_log.info(\"---------------------------------------------------------------\")\n",
    "    tile_log.info(\n",
    "        \"Change detection and report image product updating, file compression, zipping\"\n",
    "    )\n",
    "    tile_log.info(\n",
    "        \"and deletion of intermediate file products (if selected) are complete.\"\n",
    "    )\n",
    "    tile_log.info(\"---------------------------------------------------------------\")\n",
    "\n",
    "    if config_dict[\"do_delete\"]:\n",
    "        tile_log.info(\"---------------------------------------------------------------\")\n",
    "        tile_log.info(\"Deleting temporary directories starting with 'tmp*'\")\n",
    "        tile_log.info(\"These can be left over from interrupted processing runs.\")\n",
    "        tile_log.info(\"---------------------------------------------------------------\")\n",
    "        directory = tile_root_dir\n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            temp_dirs = [d for d in dirs if d.startswith(\"tmp\")]\n",
    "            for temp_dir in temp_dirs:\n",
    "                tile_log.info(\"Deleting {}\".format(os.path.join(root, temp_dir)))\n",
    "                if os.path.isdir(os.path.join(directory, f)):\n",
    "                    shutil.rmtree(os.path.join(directory, f))\n",
    "                else:\n",
    "                    tile_log.warning(\n",
    "                        \"This should not have happened. {} is not a directory. Skipping deletion.\".format(\n",
    "                            os.path.join(root, temp_dir)\n",
    "                        )\n",
    "                    )\n",
    "        tile_log.info(\"---------------------------------------------------------------\")\n",
    "        tile_log.info(\"Deletion of temporary directories complete.\")\n",
    "        tile_log.info(\"---------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "tile_log.info(\"---------------------------------------------------------------\")\n",
    "tile_log.info(\"---                  PROCESSING END                         ---\")\n",
    "tile_log.info(\"---------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600fd411-5ce2-4307-b686-b2059d8ad367",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyeo_1_env",
   "language": "python",
   "name": "pyeo_1_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
